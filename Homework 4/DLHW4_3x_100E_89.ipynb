{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nia9qnBerH12",
        "outputId": "630b484d-e311-4aa7-f193-1728f972403b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Data:\n",
            "Batch 1\n",
            "Batch 2\n",
            "Batch 3\n",
            "Batch 4\n",
            "Batch 5\n",
            "Model: \"rav_net\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization_31 (Bat  (None, 32, 32, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_30 (Conv2D)          (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_32 (Bat  (None, 32, 32, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 16, 16, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_31 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_33 (Bat  (None, 16, 16, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_32 (Conv2D)          (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_34 (Bat  (None, 16, 16, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 8, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_33 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_35 (Bat  (None, 8, 8, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_34 (Conv2D)          (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_36 (Bat  (None, 8, 8, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, 4, 4, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " conv2d_35 (Conv2D)          (None, 4, 4, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_37 (Bat  (None, 4, 4, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_36 (Conv2D)          (None, 4, 4, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_38 (Bat  (None, 4, 4, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 2, 2, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 2, 2, 128)         0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " batch_normalization_39 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 854,826\n",
            "Trainable params: 852,394\n",
            "Non-trainable params: 2,432\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "312/312 [==============================] - 24s 72ms/step - loss: 1.8211 - accuracy: 0.3685 - val_loss: 1.6898 - val_accuracy: 0.4376 - lr: 0.0050\n",
            "Epoch 2/100\n",
            "312/312 [==============================] - 21s 67ms/step - loss: 1.2568 - accuracy: 0.5539 - val_loss: 2.8111 - val_accuracy: 0.4543 - lr: 0.0049\n",
            "Epoch 3/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.9941 - accuracy: 0.6492 - val_loss: 1.1458 - val_accuracy: 0.6437 - lr: 0.0049\n",
            "Epoch 4/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.8324 - accuracy: 0.7098 - val_loss: 0.8187 - val_accuracy: 0.7360 - lr: 0.0049\n",
            "Epoch 5/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.7466 - accuracy: 0.7427 - val_loss: 0.7781 - val_accuracy: 0.7409 - lr: 0.0048\n",
            "Epoch 6/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.6815 - accuracy: 0.7642 - val_loss: 0.7725 - val_accuracy: 0.7564 - lr: 0.0047\n",
            "Epoch 7/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.6251 - accuracy: 0.7859 - val_loss: 0.7704 - val_accuracy: 0.7536 - lr: 0.0047\n",
            "Epoch 8/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.5867 - accuracy: 0.7974 - val_loss: 0.6688 - val_accuracy: 0.7825 - lr: 0.0047\n",
            "Epoch 9/100\n",
            "312/312 [==============================] - 21s 67ms/step - loss: 0.5568 - accuracy: 0.8091 - val_loss: 0.5926 - val_accuracy: 0.8030 - lr: 0.0046\n",
            "Epoch 10/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.5245 - accuracy: 0.8204 - val_loss: 0.6407 - val_accuracy: 0.7971 - lr: 0.0046\n",
            "Epoch 11/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.4963 - accuracy: 0.8289 - val_loss: 0.6169 - val_accuracy: 0.8050 - lr: 9.0000e-04\n",
            "Epoch 12/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.4792 - accuracy: 0.8356 - val_loss: 0.5531 - val_accuracy: 0.8188 - lr: 0.0044\n",
            "Epoch 13/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.4628 - accuracy: 0.8411 - val_loss: 0.4802 - val_accuracy: 0.8418 - lr: 0.0044\n",
            "Epoch 14/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.4444 - accuracy: 0.8464 - val_loss: 0.5186 - val_accuracy: 0.8319 - lr: 0.0044\n",
            "Epoch 15/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.4254 - accuracy: 0.8540 - val_loss: 0.5235 - val_accuracy: 0.8286 - lr: 8.6000e-04\n",
            "Epoch 16/100\n",
            "312/312 [==============================] - 21s 67ms/step - loss: 0.4126 - accuracy: 0.8572 - val_loss: 0.4987 - val_accuracy: 0.8375 - lr: 0.0043\n",
            "Epoch 17/100\n",
            "312/312 [==============================] - 21s 67ms/step - loss: 0.3942 - accuracy: 0.8628 - val_loss: 0.4448 - val_accuracy: 0.8546 - lr: 0.0042\n",
            "Epoch 18/100\n",
            "312/312 [==============================] - 21s 67ms/step - loss: 0.3806 - accuracy: 0.8679 - val_loss: 0.5290 - val_accuracy: 0.8355 - lr: 0.0041\n",
            "Epoch 19/100\n",
            "312/312 [==============================] - 21s 67ms/step - loss: 0.3640 - accuracy: 0.8748 - val_loss: 0.4864 - val_accuracy: 0.8460 - lr: 8.2000e-04\n",
            "Epoch 20/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.3538 - accuracy: 0.8791 - val_loss: 0.5224 - val_accuracy: 0.8386 - lr: 0.0041\n",
            "Epoch 21/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.3417 - accuracy: 0.8820 - val_loss: 0.4692 - val_accuracy: 0.8494 - lr: 8.0000e-04\n",
            "Epoch 22/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.3355 - accuracy: 0.8852 - val_loss: 0.4161 - val_accuracy: 0.8592 - lr: 0.0039\n",
            "Epoch 23/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.3230 - accuracy: 0.8886 - val_loss: 0.4743 - val_accuracy: 0.8503 - lr: 0.0039\n",
            "Epoch 24/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.3097 - accuracy: 0.8939 - val_loss: 0.4990 - val_accuracy: 0.8382 - lr: 7.7000e-04\n",
            "Epoch 25/100\n",
            "312/312 [==============================] - 20s 66ms/step - loss: 0.3021 - accuracy: 0.8951 - val_loss: 0.4334 - val_accuracy: 0.8633 - lr: 0.0038\n",
            "Epoch 26/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.2934 - accuracy: 0.8976 - val_loss: 0.4327 - val_accuracy: 0.8612 - lr: 7.5000e-04\n",
            "Epoch 27/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.2863 - accuracy: 0.9005 - val_loss: 0.4271 - val_accuracy: 0.8650 - lr: 0.0037\n",
            "Epoch 28/100\n",
            "312/312 [==============================] - 21s 67ms/step - loss: 0.2780 - accuracy: 0.9034 - val_loss: 0.4713 - val_accuracy: 0.8555 - lr: 7.3000e-04\n",
            "Epoch 29/100\n",
            "312/312 [==============================] - 21s 67ms/step - loss: 0.2704 - accuracy: 0.9056 - val_loss: 0.4413 - val_accuracy: 0.8611 - lr: 0.0036\n",
            "Epoch 30/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.2571 - accuracy: 0.9106 - val_loss: 0.4114 - val_accuracy: 0.8694 - lr: 0.0036\n",
            "Epoch 31/100\n",
            "312/312 [==============================] - 21s 67ms/step - loss: 0.2563 - accuracy: 0.9122 - val_loss: 0.3948 - val_accuracy: 0.8757 - lr: 0.0035\n",
            "Epoch 32/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.2445 - accuracy: 0.9162 - val_loss: 0.4449 - val_accuracy: 0.8687 - lr: 0.0034\n",
            "Epoch 33/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.2381 - accuracy: 0.9184 - val_loss: 0.4768 - val_accuracy: 0.8575 - lr: 6.8000e-04\n",
            "Epoch 34/100\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.2295 - accuracy: 0.9191 - val_loss: 0.4346 - val_accuracy: 0.8688 - lr: 0.0033\n",
            "Epoch 35/100\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2273 - accuracy: 0.9208 - val_loss: 0.3953 - val_accuracy: 0.8765 - lr: 6.6000e-04\n",
            "Epoch 36/100\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2190 - accuracy: 0.9229 - val_loss: 0.4160 - val_accuracy: 0.8742 - lr: 0.0033\n",
            "Epoch 37/100\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2168 - accuracy: 0.9240 - val_loss: 0.4305 - val_accuracy: 0.8690 - lr: 6.4000e-04\n",
            "Epoch 38/100\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2083 - accuracy: 0.9297 - val_loss: 0.3946 - val_accuracy: 0.8799 - lr: 0.0032\n",
            "Epoch 39/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.2011 - accuracy: 0.9303 - val_loss: 0.4010 - val_accuracy: 0.8786 - lr: 0.0031\n",
            "Epoch 40/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1922 - accuracy: 0.9333 - val_loss: 0.4747 - val_accuracy: 0.8644 - lr: 6.1000e-04\n",
            "Epoch 41/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1941 - accuracy: 0.9308 - val_loss: 0.4312 - val_accuracy: 0.8695 - lr: 0.0030\n",
            "Epoch 42/100\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1821 - accuracy: 0.9366 - val_loss: 0.4476 - val_accuracy: 0.8699 - lr: 5.9000e-04\n",
            "Epoch 43/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1842 - accuracy: 0.9360 - val_loss: 0.3971 - val_accuracy: 0.8778 - lr: 0.0029\n",
            "Epoch 44/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1759 - accuracy: 0.9376 - val_loss: 0.3926 - val_accuracy: 0.8798 - lr: 0.0029\n",
            "Epoch 45/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1670 - accuracy: 0.9417 - val_loss: 0.4537 - val_accuracy: 0.8713 - lr: 0.0028\n",
            "Epoch 46/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1672 - accuracy: 0.9410 - val_loss: 0.4288 - val_accuracy: 0.8774 - lr: 5.5000e-04\n",
            "Epoch 47/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1664 - accuracy: 0.9419 - val_loss: 0.3853 - val_accuracy: 0.8844 - lr: 0.0027\n",
            "Epoch 48/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1585 - accuracy: 0.9438 - val_loss: 0.3806 - val_accuracy: 0.8860 - lr: 0.0026\n",
            "Epoch 49/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1476 - accuracy: 0.9487 - val_loss: 0.4344 - val_accuracy: 0.8815 - lr: 0.0026\n",
            "Epoch 50/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1446 - accuracy: 0.9502 - val_loss: 0.4536 - val_accuracy: 0.8691 - lr: 5.1000e-04\n",
            "Epoch 51/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1449 - accuracy: 0.9488 - val_loss: 0.4093 - val_accuracy: 0.8801 - lr: 0.0025\n",
            "Epoch 52/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1427 - accuracy: 0.9506 - val_loss: 0.4587 - val_accuracy: 0.8733 - lr: 4.9000e-04\n",
            "Epoch 53/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1373 - accuracy: 0.9532 - val_loss: 0.4132 - val_accuracy: 0.8819 - lr: 0.0024\n",
            "Epoch 54/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1355 - accuracy: 0.9531 - val_loss: 0.4186 - val_accuracy: 0.8814 - lr: 4.7000e-04\n",
            "Epoch 55/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1311 - accuracy: 0.9543 - val_loss: 0.3907 - val_accuracy: 0.8874 - lr: 0.0023\n",
            "Epoch 56/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1246 - accuracy: 0.9570 - val_loss: 0.4508 - val_accuracy: 0.8789 - lr: 4.5000e-04\n",
            "Epoch 57/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1219 - accuracy: 0.9575 - val_loss: 0.4263 - val_accuracy: 0.8787 - lr: 0.0022\n",
            "Epoch 58/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1215 - accuracy: 0.9573 - val_loss: 0.4135 - val_accuracy: 0.8875 - lr: 4.3000e-04\n",
            "Epoch 59/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1198 - accuracy: 0.9585 - val_loss: 0.4569 - val_accuracy: 0.8785 - lr: 0.0021\n",
            "Epoch 60/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1132 - accuracy: 0.9603 - val_loss: 0.3948 - val_accuracy: 0.8918 - lr: 4.1000e-04\n",
            "Epoch 61/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1107 - accuracy: 0.9621 - val_loss: 0.4509 - val_accuracy: 0.8853 - lr: 0.0020\n",
            "Epoch 62/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1114 - accuracy: 0.9620 - val_loss: 0.4509 - val_accuracy: 0.8826 - lr: 3.9000e-04\n",
            "Epoch 63/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.1095 - accuracy: 0.9618 - val_loss: 0.4300 - val_accuracy: 0.8850 - lr: 0.0019\n",
            "Epoch 64/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1034 - accuracy: 0.9639 - val_loss: 0.4766 - val_accuracy: 0.8773 - lr: 3.7000e-04\n",
            "Epoch 65/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1001 - accuracy: 0.9653 - val_loss: 0.4380 - val_accuracy: 0.8867 - lr: 0.0018\n",
            "Epoch 66/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0952 - accuracy: 0.9677 - val_loss: 0.4414 - val_accuracy: 0.8870 - lr: 3.5000e-04\n",
            "Epoch 67/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.0961 - accuracy: 0.9667 - val_loss: 0.4208 - val_accuracy: 0.8912 - lr: 0.0017\n",
            "Epoch 68/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0953 - accuracy: 0.9673 - val_loss: 0.4259 - val_accuracy: 0.8870 - lr: 3.3000e-04\n",
            "Epoch 69/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0908 - accuracy: 0.9682 - val_loss: 0.4213 - val_accuracy: 0.8879 - lr: 0.0016\n",
            "Epoch 70/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0853 - accuracy: 0.9704 - val_loss: 0.4337 - val_accuracy: 0.8886 - lr: 3.1000e-04\n",
            "Epoch 71/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.0825 - accuracy: 0.9708 - val_loss: 0.4352 - val_accuracy: 0.8912 - lr: 0.0015\n",
            "Epoch 72/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.0832 - accuracy: 0.9701 - val_loss: 0.4482 - val_accuracy: 0.8861 - lr: 2.9000e-04\n",
            "Epoch 73/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0837 - accuracy: 0.9707 - val_loss: 0.4678 - val_accuracy: 0.8835 - lr: 0.0014\n",
            "Epoch 74/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0791 - accuracy: 0.9727 - val_loss: 0.4594 - val_accuracy: 0.8858 - lr: 2.7000e-04\n",
            "Epoch 75/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.0776 - accuracy: 0.9727 - val_loss: 0.4498 - val_accuracy: 0.8900 - lr: 0.0013\n",
            "Epoch 76/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0732 - accuracy: 0.9750 - val_loss: 0.4511 - val_accuracy: 0.8889 - lr: 2.5000e-04\n",
            "Epoch 77/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0714 - accuracy: 0.9745 - val_loss: 0.4473 - val_accuracy: 0.8908 - lr: 0.0012\n",
            "Epoch 78/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.0727 - accuracy: 0.9747 - val_loss: 0.4438 - val_accuracy: 0.8916 - lr: 2.3000e-04\n",
            "Epoch 79/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0682 - accuracy: 0.9774 - val_loss: 0.4279 - val_accuracy: 0.8962 - lr: 0.0011\n",
            "Epoch 80/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0643 - accuracy: 0.9781 - val_loss: 0.4447 - val_accuracy: 0.8966 - lr: 2.1000e-04\n",
            "Epoch 81/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0652 - accuracy: 0.9774 - val_loss: 0.4609 - val_accuracy: 0.8902 - lr: 0.0010\n",
            "Epoch 82/100\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.0624 - accuracy: 0.9775 - val_loss: 0.4506 - val_accuracy: 0.8930 - lr: 1.9000e-04\n",
            "Epoch 83/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0614 - accuracy: 0.9792 - val_loss: 0.4455 - val_accuracy: 0.8936 - lr: 9.0000e-04\n",
            "Epoch 84/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0607 - accuracy: 0.9787 - val_loss: 0.4369 - val_accuracy: 0.8958 - lr: 1.7000e-04\n",
            "Epoch 85/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0558 - accuracy: 0.9812 - val_loss: 0.4594 - val_accuracy: 0.8933 - lr: 8.0000e-04\n",
            "Epoch 86/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0542 - accuracy: 0.9815 - val_loss: 0.4494 - val_accuracy: 0.8953 - lr: 1.5000e-04\n",
            "Epoch 87/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0543 - accuracy: 0.9815 - val_loss: 0.4422 - val_accuracy: 0.8969 - lr: 7.0000e-04\n",
            "Epoch 88/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0515 - accuracy: 0.9820 - val_loss: 0.4711 - val_accuracy: 0.8936 - lr: 1.3000e-04\n",
            "Epoch 89/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0517 - accuracy: 0.9820 - val_loss: 0.4596 - val_accuracy: 0.8945 - lr: 6.0000e-04\n",
            "Epoch 90/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0500 - accuracy: 0.9828 - val_loss: 0.4612 - val_accuracy: 0.8973 - lr: 1.1000e-04\n",
            "Epoch 91/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0475 - accuracy: 0.9835 - val_loss: 0.4625 - val_accuracy: 0.8971 - lr: 5.0000e-04\n",
            "Epoch 92/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.0451 - accuracy: 0.9848 - val_loss: 0.4692 - val_accuracy: 0.8945 - lr: 9.0000e-05\n",
            "Epoch 93/100\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.0428 - accuracy: 0.9854 - val_loss: 0.4495 - val_accuracy: 0.9002 - lr: 4.0000e-04\n",
            "Epoch 94/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0450 - accuracy: 0.9848 - val_loss: 0.4580 - val_accuracy: 0.8974 - lr: 7.0000e-05\n",
            "Epoch 95/100\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.0406 - accuracy: 0.9858 - val_loss: 0.4500 - val_accuracy: 0.9000 - lr: 3.0000e-04\n",
            "Epoch 96/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0422 - accuracy: 0.9854 - val_loss: 0.4502 - val_accuracy: 0.8999 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0400 - accuracy: 0.9855 - val_loss: 0.4477 - val_accuracy: 0.9010 - lr: 2.0000e-04\n",
            "Epoch 98/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0406 - accuracy: 0.9859 - val_loss: 0.4535 - val_accuracy: 0.8990 - lr: 3.0000e-05\n",
            "Epoch 99/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0392 - accuracy: 0.9867 - val_loss: 0.4566 - val_accuracy: 0.8995 - lr: 1.0000e-04\n",
            "Epoch 100/100\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.0388 - accuracy: 0.9868 - val_loss: 0.4544 - val_accuracy: 0.9004 - lr: 1.0000e-05\n",
            "Test loss: 0.49973905086517334\n",
            "Test accuracy: 0.8932999968528748\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, MaxPool2D, Flatten, Dense, Activation, BatchNormalization, Lambda, PReLU, LeakyReLU, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
        "from tensorflow.keras.layers import Flatten, Input, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\"\"\"\n",
        "The CIFAR10 dataset was downloaded through the official website, with each training batch being unpickled and then appended to each other\n",
        "to create one large training set. The images were preprocessed to convert the initial row vector to shape (32, 32, 3) through reshaping and transposing.\n",
        "The class output data was one hot encoded. My preliminary attempt used my model for the MNIST dataset, with an alteration for the input size. This\n",
        "resulted in a test accuracy of 67% after 10 epochs. My next attempt was a VGG with fractional max pooling, based on a paper by Benjamin Graham. While this definitely \n",
        "outperformed the previous model, the computational time was far too high. I tried reducing training time by using only one of the 5 training batches and doubling the \n",
        "batch size to 512, but the tradeoff with accuracy was way too high. \n",
        "\n",
        "The next model I tried was another VGG type convolutional network, which was shallower and converged much faster. This got me to 80% test accuracy.   \n",
        "\"\"\"\n",
        "# Fractional max pooling\n",
        "# - https://arxiv.org/abs/1412.6071\n",
        "# - https://github.com/laplacetw/vgg-like-cifar10/blob/master/fmp_cifar10.py\n",
        "# https://www.binarystudy.com/2021/09/how-to-load-preprocess-visualize-CIFAR-10-and-CIFAR-100.html#routine\n",
        "\n",
        "BTEST = '../data/test_batch'\n",
        "meta_file = '../CIFAR10-data/batches.meta'\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "NUM_TRAINING_BATCHES = 5\n",
        "BATCH_SIZE = 128 #128\n",
        "LAMBDA = 1e-5\n",
        "EPOCHS = 100\n",
        "IMG_SIDE_LEN = 32\n",
        "LR = 5e-3\n",
        "DATASET = \"CIFAR10\"\n",
        "MODEL = \"model3x\"\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        u = pickle._Unpickler( fo )\n",
        "        u.encoding = 'latin1'\n",
        "        dict = u.load()\n",
        "    return dict\n",
        "\n",
        "def load_training_data():\n",
        "    # The whole data_batch_1 has 10,000 images. And each image is a 1-D array having 3,072 entries. \n",
        "    # First 1024 entries for Red, the next 1024 entries for Green and last 1024 entries for Blue channels. \n",
        "    print(\"Loading Data:\")\n",
        "    features, classes = np.empty((0,32,32,3)), np.empty((0,10))\n",
        "    for i in range(NUM_TRAINING_BATCHES):\n",
        "        print(f\"Batch {i+1}\")\n",
        "        batch_path = f'../data/data_batch_{i+1}'\n",
        "        x, y = reshape_features(batch_path)\n",
        "        features = np.append(features, x, axis=0)\n",
        "        classes = np.append(classes, y, axis=0)\n",
        "    return features, classes\n",
        "\n",
        "def reshape_features(feat_path, CIFAR100=False):\n",
        "    labels = 'coarse_labels' if CIFAR100 else 'labels'\n",
        "    unpickled_data = unpickle(feat_path)\n",
        "    return (unpickled_data['data'].reshape(len(unpickled_data['data']),3,32,32).transpose(0,2,3,1) / 255,\n",
        "            tf.keras.utils.to_categorical(unpickled_data[labels]))\n",
        "\n",
        "def frac_max_pool(x):\n",
        "    return tf.nn.fractional_max_pool(x, [1.0, 1.41, 1.41, 1.0], pseudo_random=True, overlapping=True)[0]\n",
        "\n",
        "def poly_decay(epoch):\n",
        "  maxEpochs = EPOCHS\n",
        "  baseLR = LR\n",
        "  power = 1.0\n",
        "  alpha = baseLR * (1 - (epoch   / float(maxEpochs))) ** power\n",
        "  return alpha\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    horizontal_flip=True,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        "    #zoom_range=0.3\n",
        ")\n",
        "\n",
        "aug = ImageDataGenerator(width_shift_range=0.1,height_shift_range=0.1, horizontal_flip=True,fill_mode=\"nearest\")\n",
        "\n",
        "\n",
        "def normalize_x_data(x_train, x_test):\n",
        "    eps = 1e-7\n",
        "    mean = np.mean(x_train,axis = (0, 1, 2, 3))\n",
        "    std = np.std(x_train,axis = (0, 1, 2, 3))\n",
        "    x_train = (x_train - mean)/(std + eps)\n",
        "    x_test = (x_test - mean)/(std + eps)\n",
        "    return x_train, x_test\n",
        "\n",
        "class Data10(object):\n",
        "    def __init__(self):\n",
        "        self.x_train, self.y_train = load_training_data()\n",
        "        self.x_test, self.y_test = reshape_features(BTEST)\n",
        "        self.x_train, self.x_test = normalize_x_data(self.x_train, self.x_test)\n",
        "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train, self.y_train, test_size=0.2, random_state=31415)\n",
        "\n",
        "class Data100(object):\n",
        "    def __init__(self):\n",
        "        self.x_train, self.y_train = reshape_features('../CIFAR100-data/train', CIFAR100=True)\n",
        "        self.x_test, self.y_test = reshape_features('../CIFAR100-data/test', CIFAR100=True)\n",
        "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train, self.y_train, test_size=0.2, random_state=31415)\n",
        "\n",
        "def double_conv_module(input, num_filters, activation, kern_reg, dropout, padding=\"same\"):\n",
        "    input = Conv2D(filters = num_filters, kernel_size = (3, 3), activation = activation, padding = padding, kernel_regularizer = kern_reg)(input)\n",
        "    input = BatchNormalization(axis=-1)(input)\n",
        "    input = Conv2D(filters = num_filters, kernel_size = (3, 3), activation = activation, padding = padding, kernel_regularizer = kern_reg)(input)\n",
        "    input = BatchNormalization(axis=-1)(input)\n",
        "    input = MaxPooling2D(pool_size = (2, 2))(input)\n",
        "    input = Dropout(dropout)(input)\n",
        "\n",
        "    return input\n",
        "\n",
        "def rav_model(width, height, depth, classes):\n",
        "    inputShape=(height, width, depth)\n",
        "    weight_decay = 0.001\n",
        "\n",
        "    # (Step 1) Define the model input\n",
        "    inputs = Input(shape=inputShape)\n",
        "    KR = None #l2(weight_decay)\n",
        "    x = double_conv_module(inputs, 32, activation='relu', kern_reg=KR, dropout = 0, padding='same')\n",
        "    x = double_conv_module(x, 64, activation='relu', kern_reg=KR, dropout = 0, padding='same')\n",
        "    x = double_conv_module(x, 128, activation='relu', kern_reg=KR, dropout = 0, padding='same')\n",
        "    x = double_conv_module(x, 128, activation='relu', kern_reg=KR, dropout = 0, padding='same')\n",
        "   \n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu',kernel_regularizer=None)(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(classes)(x)\n",
        "    x = Activation(\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, x, name=\"rav_net\")\n",
        "    return model\n",
        "\n",
        "def fmp_unit(input, num_filters, dropout, padding=\"same\", frac_pool=True):\n",
        "    input = Conv2D(filters = num_filters, kernel_size = (3, 3), padding = padding, kernel_initializer='he_uniform')(input)\n",
        "    input = LeakyReLU()(input)\n",
        "    input = BatchNormalization(axis=-1)(input)\n",
        "    input = Conv2D(filters = num_filters, kernel_size = (3, 3), padding = padding, kernel_initializer='he_uniform')(input)\n",
        "    input = LeakyReLU()(input)\n",
        "    input = BatchNormalization(axis=-1)(input)\n",
        "    input = Lambda(frac_max_pool)(input) if frac_pool else input\n",
        "    input = Dropout(dropout)(input)\n",
        "\n",
        "    return input\n",
        "\n",
        "def fmp_model(width, height, depth, classes):\n",
        "    inputShape=(height, width, depth)\n",
        "    inputs = Input(shape=inputShape)\n",
        "\n",
        "    x = fmp_unit(inputs, 32, dropout = 0.3, padding='same', frac_pool=False)\n",
        "    x = fmp_unit(x, 64, dropout = 0.35, padding='same')\n",
        "    x = fmp_unit(x, 96, dropout = 0.35, padding='same')\n",
        "    x = fmp_unit(x, 128, dropout = 0.4, padding='same')\n",
        "    x = fmp_unit(x, 160, dropout = 0.45, padding='same')\n",
        "    x = fmp_unit(x, 192, dropout = 0.5, padding='same')\n",
        "\n",
        "    x = Conv2D(filters=192, kernel_size=(1, 1), padding='same', kernel_initializer='he_uniform')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(units=classes, kernel_initializer='he_uniform')(x)\n",
        "    x = Activation(\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, x, name=\"fmp_rav_net\")\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if DATASET == 'CIFAR10':\n",
        "        data = Data10()\n",
        "    else:\n",
        "        data = Data100()\n",
        "\n",
        "    x_train, y_train = data.x_train, data.y_train\n",
        "    x_test, y_test = data.x_test, data.y_test\n",
        "    x_val, y_val = data.x_val, data.y_val\n",
        "\n",
        "    lr_scheduler = LearningRateScheduler(poly_decay)\n",
        "    variable_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor = 0.2, patience = 2)\n",
        "\n",
        "    if MODEL == \"model3x\":\n",
        "      model3x = rav_model(width=32, height=32, depth=3, classes=10)\n",
        "      ac='relu'\n",
        "      adm=Adam(learning_rate=0.001,decay=0, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "      opt=adm\n",
        "      model3x.compile(loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'],optimizer=opt)\n",
        "      model3x.summary()\n",
        "    \n",
        "      history=model3x.fit(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE), \n",
        "                        batch_size=BATCH_SIZE, \n",
        "                        epochs=EPOCHS, \n",
        "                        callbacks=[variable_learning_rate, lr_scheduler], \n",
        "                        validation_data=(x_val, y_val), \n",
        "                        verbose=1, \n",
        "                        steps_per_epoch = len(x_train) // BATCH_SIZE)\n",
        "      score = model3x.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    elif MODEL == \"fmp\":\n",
        "      model = fmp_model(width=32, height=32, depth=3, classes=10)\n",
        "      opt = RMSprop(decay=1e-6)\n",
        "      model.compile(loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'],optimizer=opt)\n",
        "      model.summary()\n",
        "      history=model.fit(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE), \n",
        "                        batch_size=BATCH_SIZE, \n",
        "                        epochs=EPOCHS, \n",
        "                        callbacks=[lr_scheduler], \n",
        "                        validation_data=(x_val, y_val), \n",
        "                        verbose=1, \n",
        "                        steps_per_epoch = len(x_train) // BATCH_SIZE)\n",
        "      score = model.evaluate(x_test, y_test, verbose=0)\n",
        "      \n",
        "\n",
        "    print('Test loss:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "\n",
        "    # 30 epochs 82%"
      ]
    }
  ]
}