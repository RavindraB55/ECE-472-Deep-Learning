{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nia9qnBerH12",
        "outputId": "04b0ffe3-f3f9-477d-b5ed-23e4e0ff93be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Data:\n",
            "Batch 1\n",
            "Batch 2\n",
            "Batch 3\n",
            "Batch 4\n",
            "Batch 5\n",
            "Model: \"rav_net\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d_24 (Conv2D)          (None, 32, 32, 128)       3584      \n",
            "                                                                 \n",
            " batch_normalization_25 (Bat  (None, 32, 32, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 32, 32, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_26 (Bat  (None, 32, 32, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  (None, 16, 16, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 512)               16777728  \n",
            "                                                                 \n",
            " batch_normalization_27 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,937,098\n",
            "Trainable params: 16,935,562\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n",
            "Epoch 1/175\n",
            "312/312 [==============================] - 25s 75ms/step - loss: 1.7962 - accuracy: 0.4004 - val_loss: 1.3015 - val_accuracy: 0.5336 - lr: 0.0050\n",
            "Epoch 2/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 1.3561 - accuracy: 0.5134 - val_loss: 1.1049 - val_accuracy: 0.6031 - lr: 0.0050\n",
            "Epoch 3/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 1.1844 - accuracy: 0.5775 - val_loss: 1.1410 - val_accuracy: 0.6099 - lr: 0.0049\n",
            "Epoch 4/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 1.0872 - accuracy: 0.6117 - val_loss: 1.0241 - val_accuracy: 0.6552 - lr: 0.0049\n",
            "Epoch 5/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 1.0171 - accuracy: 0.6388 - val_loss: 0.8997 - val_accuracy: 0.6851 - lr: 0.0049\n",
            "Epoch 6/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.9704 - accuracy: 0.6577 - val_loss: 0.8904 - val_accuracy: 0.6961 - lr: 0.0049\n",
            "Epoch 7/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.9349 - accuracy: 0.6701 - val_loss: 0.8352 - val_accuracy: 0.7144 - lr: 0.0048\n",
            "Epoch 8/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.9026 - accuracy: 0.6819 - val_loss: 0.9086 - val_accuracy: 0.6931 - lr: 0.0048\n",
            "Epoch 9/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.8709 - accuracy: 0.6928 - val_loss: 0.8074 - val_accuracy: 0.7242 - lr: 0.0048\n",
            "Epoch 10/175\n",
            "312/312 [==============================] - 23s 74ms/step - loss: 0.8486 - accuracy: 0.7028 - val_loss: 0.7901 - val_accuracy: 0.7259 - lr: 0.0047\n",
            "Epoch 11/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.8353 - accuracy: 0.7052 - val_loss: 0.8557 - val_accuracy: 0.7156 - lr: 0.0047\n",
            "Epoch 12/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.8111 - accuracy: 0.7134 - val_loss: 0.8070 - val_accuracy: 0.7216 - lr: 9.3714e-04\n",
            "Epoch 13/175\n",
            "312/312 [==============================] - 23s 74ms/step - loss: 0.7978 - accuracy: 0.7180 - val_loss: 0.7806 - val_accuracy: 0.7407 - lr: 0.0047\n",
            "Epoch 14/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.7868 - accuracy: 0.7226 - val_loss: 0.7822 - val_accuracy: 0.7301 - lr: 0.0046\n",
            "Epoch 15/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.7691 - accuracy: 0.7294 - val_loss: 0.7160 - val_accuracy: 0.7570 - lr: 0.0046\n",
            "Epoch 16/175\n",
            "312/312 [==============================] - 23s 74ms/step - loss: 0.7558 - accuracy: 0.7365 - val_loss: 0.8013 - val_accuracy: 0.7351 - lr: 0.0046\n",
            "Epoch 17/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.7420 - accuracy: 0.7411 - val_loss: 0.7075 - val_accuracy: 0.7566 - lr: 0.0045\n",
            "Epoch 18/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.7306 - accuracy: 0.7419 - val_loss: 0.7756 - val_accuracy: 0.7405 - lr: 0.0045\n",
            "Epoch 19/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.7118 - accuracy: 0.7526 - val_loss: 0.7205 - val_accuracy: 0.7609 - lr: 8.9714e-04\n",
            "Epoch 20/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.7065 - accuracy: 0.7531 - val_loss: 0.7251 - val_accuracy: 0.7610 - lr: 0.0045\n",
            "Epoch 21/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.6954 - accuracy: 0.7575 - val_loss: 0.7042 - val_accuracy: 0.7617 - lr: 0.0044\n",
            "Epoch 22/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.6918 - accuracy: 0.7567 - val_loss: 0.6188 - val_accuracy: 0.7850 - lr: 0.0044\n",
            "Epoch 23/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.6781 - accuracy: 0.7606 - val_loss: 0.7239 - val_accuracy: 0.7606 - lr: 0.0044\n",
            "Epoch 24/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.6726 - accuracy: 0.7668 - val_loss: 0.6932 - val_accuracy: 0.7712 - lr: 8.6857e-04\n",
            "Epoch 25/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.6589 - accuracy: 0.7685 - val_loss: 0.6490 - val_accuracy: 0.7852 - lr: 0.0043\n",
            "Epoch 26/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.6490 - accuracy: 0.7723 - val_loss: 0.6392 - val_accuracy: 0.7890 - lr: 8.5714e-04\n",
            "Epoch 27/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.6462 - accuracy: 0.7717 - val_loss: 0.6320 - val_accuracy: 0.7847 - lr: 0.0043\n",
            "Epoch 28/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.6362 - accuracy: 0.7770 - val_loss: 0.6619 - val_accuracy: 0.7818 - lr: 8.4571e-04\n",
            "Epoch 29/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.6222 - accuracy: 0.7800 - val_loss: 0.7042 - val_accuracy: 0.7661 - lr: 0.0042\n",
            "Epoch 30/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.6194 - accuracy: 0.7837 - val_loss: 0.6299 - val_accuracy: 0.7897 - lr: 8.3429e-04\n",
            "Epoch 31/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.6176 - accuracy: 0.7840 - val_loss: 0.6242 - val_accuracy: 0.7940 - lr: 0.0041\n",
            "Epoch 32/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.6013 - accuracy: 0.7902 - val_loss: 0.6119 - val_accuracy: 0.7958 - lr: 0.0041\n",
            "Epoch 33/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.5957 - accuracy: 0.7896 - val_loss: 0.6162 - val_accuracy: 0.7946 - lr: 0.0041\n",
            "Epoch 34/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.5877 - accuracy: 0.7934 - val_loss: 0.6497 - val_accuracy: 0.7894 - lr: 8.1143e-04\n",
            "Epoch 35/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.5821 - accuracy: 0.7948 - val_loss: 0.6287 - val_accuracy: 0.7913 - lr: 0.0040\n",
            "Epoch 36/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.5772 - accuracy: 0.7958 - val_loss: 0.6229 - val_accuracy: 0.7985 - lr: 8.0000e-04\n",
            "Epoch 37/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.5708 - accuracy: 0.7992 - val_loss: 0.6694 - val_accuracy: 0.7832 - lr: 0.0040\n",
            "Epoch 38/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.5642 - accuracy: 0.8012 - val_loss: 0.5993 - val_accuracy: 0.8051 - lr: 0.0039\n",
            "Epoch 39/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.5576 - accuracy: 0.8019 - val_loss: 0.6230 - val_accuracy: 0.7990 - lr: 0.0039\n",
            "Epoch 40/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.5577 - accuracy: 0.8054 - val_loss: 0.5875 - val_accuracy: 0.8065 - lr: 0.0039\n",
            "Epoch 41/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.5461 - accuracy: 0.8085 - val_loss: 0.6046 - val_accuracy: 0.8013 - lr: 0.0039\n",
            "Epoch 42/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.5460 - accuracy: 0.8068 - val_loss: 0.6177 - val_accuracy: 0.7940 - lr: 7.6571e-04\n",
            "Epoch 43/175\n",
            "312/312 [==============================] - 22s 69ms/step - loss: 0.5407 - accuracy: 0.8102 - val_loss: 0.6216 - val_accuracy: 0.7977 - lr: 0.0038\n",
            "Epoch 44/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.5301 - accuracy: 0.8131 - val_loss: 0.6018 - val_accuracy: 0.8027 - lr: 7.5429e-04\n",
            "Epoch 45/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.5245 - accuracy: 0.8140 - val_loss: 0.6384 - val_accuracy: 0.7924 - lr: 0.0037\n",
            "Epoch 46/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.5157 - accuracy: 0.8178 - val_loss: 0.6517 - val_accuracy: 0.7850 - lr: 7.4286e-04\n",
            "Epoch 47/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.5125 - accuracy: 0.8195 - val_loss: 0.6004 - val_accuracy: 0.8080 - lr: 0.0037\n",
            "Epoch 48/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.5034 - accuracy: 0.8220 - val_loss: 0.5846 - val_accuracy: 0.8054 - lr: 0.0037\n",
            "Epoch 49/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.5109 - accuracy: 0.8186 - val_loss: 0.5723 - val_accuracy: 0.8112 - lr: 0.0036\n",
            "Epoch 50/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.5005 - accuracy: 0.8234 - val_loss: 0.5474 - val_accuracy: 0.8187 - lr: 0.0036\n",
            "Epoch 51/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.5010 - accuracy: 0.8245 - val_loss: 0.6148 - val_accuracy: 0.8052 - lr: 0.0036\n",
            "Epoch 52/175\n",
            "312/312 [==============================] - 23s 75ms/step - loss: 0.4890 - accuracy: 0.8277 - val_loss: 0.6097 - val_accuracy: 0.8102 - lr: 7.0857e-04\n",
            "Epoch 53/175\n",
            "312/312 [==============================] - 23s 74ms/step - loss: 0.4871 - accuracy: 0.8293 - val_loss: 0.6121 - val_accuracy: 0.8019 - lr: 0.0035\n",
            "Epoch 54/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.4811 - accuracy: 0.8325 - val_loss: 0.5834 - val_accuracy: 0.8128 - lr: 6.9714e-04\n",
            "Epoch 55/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.4729 - accuracy: 0.8331 - val_loss: 0.5637 - val_accuracy: 0.8177 - lr: 0.0035\n",
            "Epoch 56/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.4740 - accuracy: 0.8336 - val_loss: 0.5772 - val_accuracy: 0.8160 - lr: 6.8571e-04\n",
            "Epoch 57/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.4695 - accuracy: 0.8338 - val_loss: 0.5948 - val_accuracy: 0.8114 - lr: 0.0034\n",
            "Epoch 58/175\n",
            "312/312 [==============================] - 23s 75ms/step - loss: 0.4665 - accuracy: 0.8361 - val_loss: 0.5474 - val_accuracy: 0.8221 - lr: 6.7429e-04\n",
            "Epoch 59/175\n",
            "312/312 [==============================] - 23s 75ms/step - loss: 0.4650 - accuracy: 0.8362 - val_loss: 0.5722 - val_accuracy: 0.8189 - lr: 0.0033\n",
            "Epoch 60/175\n",
            "312/312 [==============================] - 23s 74ms/step - loss: 0.4623 - accuracy: 0.8366 - val_loss: 0.5920 - val_accuracy: 0.8102 - lr: 6.6286e-04\n",
            "Epoch 61/175\n",
            "312/312 [==============================] - 23s 75ms/step - loss: 0.4514 - accuracy: 0.8411 - val_loss: 0.5453 - val_accuracy: 0.8226 - lr: 0.0033\n",
            "Epoch 62/175\n",
            "312/312 [==============================] - 23s 74ms/step - loss: 0.4504 - accuracy: 0.8399 - val_loss: 0.5613 - val_accuracy: 0.8177 - lr: 0.0033\n",
            "Epoch 63/175\n",
            "312/312 [==============================] - 23s 74ms/step - loss: 0.4466 - accuracy: 0.8419 - val_loss: 0.5504 - val_accuracy: 0.8220 - lr: 6.4571e-04\n",
            "Epoch 64/175\n",
            "312/312 [==============================] - 23s 74ms/step - loss: 0.4524 - accuracy: 0.8402 - val_loss: 0.5832 - val_accuracy: 0.8123 - lr: 0.0032\n",
            "Epoch 65/175\n",
            "312/312 [==============================] - 23s 74ms/step - loss: 0.4374 - accuracy: 0.8468 - val_loss: 0.5761 - val_accuracy: 0.8181 - lr: 6.3429e-04\n",
            "Epoch 66/175\n",
            "312/312 [==============================] - 23s 74ms/step - loss: 0.4374 - accuracy: 0.8454 - val_loss: 0.5720 - val_accuracy: 0.8191 - lr: 0.0031\n",
            "Epoch 67/175\n",
            "312/312 [==============================] - 23s 74ms/step - loss: 0.4384 - accuracy: 0.8459 - val_loss: 0.5397 - val_accuracy: 0.8221 - lr: 0.0031\n",
            "Epoch 68/175\n",
            "312/312 [==============================] - 23s 74ms/step - loss: 0.4331 - accuracy: 0.8477 - val_loss: 0.6004 - val_accuracy: 0.8103 - lr: 0.0031\n",
            "Epoch 69/175\n",
            "312/312 [==============================] - 23s 75ms/step - loss: 0.4264 - accuracy: 0.8480 - val_loss: 0.6285 - val_accuracy: 0.8127 - lr: 6.1143e-04\n",
            "Epoch 70/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.4284 - accuracy: 0.8497 - val_loss: 0.5584 - val_accuracy: 0.8223 - lr: 0.0030\n",
            "Epoch 71/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.4250 - accuracy: 0.8500 - val_loss: 0.5527 - val_accuracy: 0.8210 - lr: 6.0000e-04\n",
            "Epoch 72/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.4176 - accuracy: 0.8529 - val_loss: 0.5284 - val_accuracy: 0.8291 - lr: 0.0030\n",
            "Epoch 73/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.4167 - accuracy: 0.8542 - val_loss: 0.6056 - val_accuracy: 0.8107 - lr: 0.0029\n",
            "Epoch 74/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.4085 - accuracy: 0.8553 - val_loss: 0.6144 - val_accuracy: 0.8102 - lr: 5.8286e-04\n",
            "Epoch 75/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.4077 - accuracy: 0.8584 - val_loss: 0.5131 - val_accuracy: 0.8329 - lr: 0.0029\n",
            "Epoch 76/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.4061 - accuracy: 0.8549 - val_loss: 0.5794 - val_accuracy: 0.8135 - lr: 0.0029\n",
            "Epoch 77/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.4077 - accuracy: 0.8546 - val_loss: 0.5383 - val_accuracy: 0.8288 - lr: 5.6571e-04\n",
            "Epoch 78/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3996 - accuracy: 0.8574 - val_loss: 0.5392 - val_accuracy: 0.8251 - lr: 0.0028\n",
            "Epoch 79/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.4013 - accuracy: 0.8580 - val_loss: 0.5269 - val_accuracy: 0.8248 - lr: 5.5429e-04\n",
            "Epoch 80/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3884 - accuracy: 0.8633 - val_loss: 0.5791 - val_accuracy: 0.8212 - lr: 0.0027\n",
            "Epoch 81/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.3914 - accuracy: 0.8620 - val_loss: 0.5517 - val_accuracy: 0.8248 - lr: 5.4286e-04\n",
            "Epoch 82/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3857 - accuracy: 0.8646 - val_loss: 0.5425 - val_accuracy: 0.8275 - lr: 0.0027\n",
            "Epoch 83/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.3844 - accuracy: 0.8657 - val_loss: 0.5386 - val_accuracy: 0.8278 - lr: 5.3143e-04\n",
            "Epoch 84/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.3792 - accuracy: 0.8654 - val_loss: 0.6197 - val_accuracy: 0.8123 - lr: 0.0026\n",
            "Epoch 85/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3802 - accuracy: 0.8657 - val_loss: 0.5351 - val_accuracy: 0.8317 - lr: 5.2000e-04\n",
            "Epoch 86/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3744 - accuracy: 0.8677 - val_loss: 0.5163 - val_accuracy: 0.8362 - lr: 0.0026\n",
            "Epoch 87/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.3746 - accuracy: 0.8676 - val_loss: 0.5602 - val_accuracy: 0.8253 - lr: 5.0857e-04\n",
            "Epoch 88/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3736 - accuracy: 0.8685 - val_loss: 0.5429 - val_accuracy: 0.8267 - lr: 0.0025\n",
            "Epoch 89/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3661 - accuracy: 0.8706 - val_loss: 0.5961 - val_accuracy: 0.8215 - lr: 4.9714e-04\n",
            "Epoch 90/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3664 - accuracy: 0.8709 - val_loss: 0.5322 - val_accuracy: 0.8313 - lr: 0.0025\n",
            "Epoch 91/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.3697 - accuracy: 0.8692 - val_loss: 0.5417 - val_accuracy: 0.8345 - lr: 4.8571e-04\n",
            "Epoch 92/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.3681 - accuracy: 0.8709 - val_loss: 0.5297 - val_accuracy: 0.8323 - lr: 0.0024\n",
            "Epoch 93/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3551 - accuracy: 0.8732 - val_loss: 0.5785 - val_accuracy: 0.8235 - lr: 4.7429e-04\n",
            "Epoch 94/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.3586 - accuracy: 0.8748 - val_loss: 0.5563 - val_accuracy: 0.8312 - lr: 0.0023\n",
            "Epoch 95/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.3525 - accuracy: 0.8748 - val_loss: 0.5856 - val_accuracy: 0.8232 - lr: 4.6286e-04\n",
            "Epoch 96/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.3585 - accuracy: 0.8737 - val_loss: 0.5899 - val_accuracy: 0.8265 - lr: 0.0023\n",
            "Epoch 97/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3494 - accuracy: 0.8760 - val_loss: 0.5357 - val_accuracy: 0.8361 - lr: 4.5143e-04\n",
            "Epoch 98/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3485 - accuracy: 0.8763 - val_loss: 0.5710 - val_accuracy: 0.8284 - lr: 0.0022\n",
            "Epoch 99/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3445 - accuracy: 0.8784 - val_loss: 0.5924 - val_accuracy: 0.8233 - lr: 4.4000e-04\n",
            "Epoch 100/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3463 - accuracy: 0.8774 - val_loss: 0.5146 - val_accuracy: 0.8415 - lr: 0.0022\n",
            "Epoch 101/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3476 - accuracy: 0.8770 - val_loss: 0.5381 - val_accuracy: 0.8362 - lr: 4.2857e-04\n",
            "Epoch 102/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.3374 - accuracy: 0.8802 - val_loss: 0.5129 - val_accuracy: 0.8384 - lr: 0.0021\n",
            "Epoch 103/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3366 - accuracy: 0.8807 - val_loss: 0.5600 - val_accuracy: 0.8284 - lr: 0.0021\n",
            "Epoch 104/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.3368 - accuracy: 0.8824 - val_loss: 0.5111 - val_accuracy: 0.8428 - lr: 0.0021\n",
            "Epoch 105/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.3328 - accuracy: 0.8805 - val_loss: 0.5599 - val_accuracy: 0.8306 - lr: 0.0020\n",
            "Epoch 106/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.3334 - accuracy: 0.8817 - val_loss: 0.5476 - val_accuracy: 0.8333 - lr: 4.0000e-04\n",
            "Epoch 107/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3299 - accuracy: 0.8825 - val_loss: 0.5519 - val_accuracy: 0.8364 - lr: 0.0020\n",
            "Epoch 108/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3271 - accuracy: 0.8843 - val_loss: 0.5120 - val_accuracy: 0.8431 - lr: 3.8857e-04\n",
            "Epoch 109/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.3265 - accuracy: 0.8828 - val_loss: 0.5190 - val_accuracy: 0.8388 - lr: 0.0019\n",
            "Epoch 110/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3159 - accuracy: 0.8879 - val_loss: 0.5575 - val_accuracy: 0.8334 - lr: 3.7714e-04\n",
            "Epoch 111/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.3270 - accuracy: 0.8850 - val_loss: 0.5194 - val_accuracy: 0.8371 - lr: 0.0019\n",
            "Epoch 112/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.3233 - accuracy: 0.8869 - val_loss: 0.5175 - val_accuracy: 0.8429 - lr: 3.6571e-04\n",
            "Epoch 113/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.3142 - accuracy: 0.8888 - val_loss: 0.5589 - val_accuracy: 0.8311 - lr: 0.0018\n",
            "Epoch 114/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.3154 - accuracy: 0.8881 - val_loss: 0.5269 - val_accuracy: 0.8358 - lr: 3.5429e-04\n",
            "Epoch 115/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.3145 - accuracy: 0.8883 - val_loss: 0.5543 - val_accuracy: 0.8315 - lr: 0.0017\n",
            "Epoch 116/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.3155 - accuracy: 0.8892 - val_loss: 0.5366 - val_accuracy: 0.8386 - lr: 3.4286e-04\n",
            "Epoch 117/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.3096 - accuracy: 0.8913 - val_loss: 0.5435 - val_accuracy: 0.8333 - lr: 0.0017\n",
            "Epoch 118/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.3089 - accuracy: 0.8911 - val_loss: 0.5741 - val_accuracy: 0.8332 - lr: 3.3143e-04\n",
            "Epoch 119/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2990 - accuracy: 0.8940 - val_loss: 0.5035 - val_accuracy: 0.8452 - lr: 0.0016\n",
            "Epoch 120/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.2995 - accuracy: 0.8945 - val_loss: 0.5410 - val_accuracy: 0.8327 - lr: 0.0016\n",
            "Epoch 121/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2973 - accuracy: 0.8944 - val_loss: 0.5574 - val_accuracy: 0.8342 - lr: 3.1429e-04\n",
            "Epoch 122/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.2990 - accuracy: 0.8932 - val_loss: 0.5378 - val_accuracy: 0.8395 - lr: 0.0015\n",
            "Epoch 123/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2984 - accuracy: 0.8955 - val_loss: 0.5094 - val_accuracy: 0.8458 - lr: 3.0286e-04\n",
            "Epoch 124/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2962 - accuracy: 0.8952 - val_loss: 0.5509 - val_accuracy: 0.8363 - lr: 0.0015\n",
            "Epoch 125/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.2930 - accuracy: 0.8946 - val_loss: 0.5280 - val_accuracy: 0.8415 - lr: 2.9143e-04\n",
            "Epoch 126/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2924 - accuracy: 0.8972 - val_loss: 0.5400 - val_accuracy: 0.8401 - lr: 0.0014\n",
            "Epoch 127/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2885 - accuracy: 0.8994 - val_loss: 0.5150 - val_accuracy: 0.8421 - lr: 2.8000e-04\n",
            "Epoch 128/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2801 - accuracy: 0.8997 - val_loss: 0.5581 - val_accuracy: 0.8369 - lr: 0.0014\n",
            "Epoch 129/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2883 - accuracy: 0.8981 - val_loss: 0.5381 - val_accuracy: 0.8384 - lr: 2.6857e-04\n",
            "Epoch 130/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.2860 - accuracy: 0.8991 - val_loss: 0.5592 - val_accuracy: 0.8378 - lr: 0.0013\n",
            "Epoch 131/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2835 - accuracy: 0.8993 - val_loss: 0.5332 - val_accuracy: 0.8442 - lr: 2.5714e-04\n",
            "Epoch 132/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.2901 - accuracy: 0.8986 - val_loss: 0.5382 - val_accuracy: 0.8413 - lr: 0.0013\n",
            "Epoch 133/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.2797 - accuracy: 0.9009 - val_loss: 0.5208 - val_accuracy: 0.8431 - lr: 2.4571e-04\n",
            "Epoch 134/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.2789 - accuracy: 0.9012 - val_loss: 0.5212 - val_accuracy: 0.8413 - lr: 0.0012\n",
            "Epoch 135/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2736 - accuracy: 0.9020 - val_loss: 0.5351 - val_accuracy: 0.8420 - lr: 2.3429e-04\n",
            "Epoch 136/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.2738 - accuracy: 0.9039 - val_loss: 0.5854 - val_accuracy: 0.8365 - lr: 0.0011\n",
            "Epoch 137/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2717 - accuracy: 0.9037 - val_loss: 0.5288 - val_accuracy: 0.8451 - lr: 2.2286e-04\n",
            "Epoch 138/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.2758 - accuracy: 0.9030 - val_loss: 0.5295 - val_accuracy: 0.8446 - lr: 0.0011\n",
            "Epoch 139/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2765 - accuracy: 0.9038 - val_loss: 0.5194 - val_accuracy: 0.8406 - lr: 2.1143e-04\n",
            "Epoch 140/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2683 - accuracy: 0.9062 - val_loss: 0.5220 - val_accuracy: 0.8440 - lr: 0.0010\n",
            "Epoch 141/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2721 - accuracy: 0.9035 - val_loss: 0.4927 - val_accuracy: 0.8489 - lr: 0.0010\n",
            "Epoch 142/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2704 - accuracy: 0.9028 - val_loss: 0.5167 - val_accuracy: 0.8458 - lr: 9.7143e-04\n",
            "Epoch 143/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.2677 - accuracy: 0.9055 - val_loss: 0.5391 - val_accuracy: 0.8408 - lr: 1.8857e-04\n",
            "Epoch 144/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2645 - accuracy: 0.9066 - val_loss: 0.5207 - val_accuracy: 0.8461 - lr: 9.1429e-04\n",
            "Epoch 145/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.2684 - accuracy: 0.9051 - val_loss: 0.5272 - val_accuracy: 0.8455 - lr: 1.7714e-04\n",
            "Epoch 146/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.2610 - accuracy: 0.9089 - val_loss: 0.5380 - val_accuracy: 0.8420 - lr: 8.5714e-04\n",
            "Epoch 147/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2671 - accuracy: 0.9053 - val_loss: 0.5108 - val_accuracy: 0.8472 - lr: 1.6571e-04\n",
            "Epoch 148/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.2602 - accuracy: 0.9093 - val_loss: 0.5277 - val_accuracy: 0.8451 - lr: 8.0000e-04\n",
            "Epoch 149/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2544 - accuracy: 0.9102 - val_loss: 0.5296 - val_accuracy: 0.8427 - lr: 1.5429e-04\n",
            "Epoch 150/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2541 - accuracy: 0.9092 - val_loss: 0.5309 - val_accuracy: 0.8441 - lr: 7.4286e-04\n",
            "Epoch 151/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.2554 - accuracy: 0.9094 - val_loss: 0.5318 - val_accuracy: 0.8415 - lr: 1.4286e-04\n",
            "Epoch 152/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.2544 - accuracy: 0.9096 - val_loss: 0.5133 - val_accuracy: 0.8463 - lr: 6.8571e-04\n",
            "Epoch 153/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2572 - accuracy: 0.9095 - val_loss: 0.5277 - val_accuracy: 0.8469 - lr: 1.3143e-04\n",
            "Epoch 154/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.2536 - accuracy: 0.9108 - val_loss: 0.5188 - val_accuracy: 0.8439 - lr: 6.2857e-04\n",
            "Epoch 155/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.2486 - accuracy: 0.9113 - val_loss: 0.5289 - val_accuracy: 0.8472 - lr: 1.2000e-04\n",
            "Epoch 156/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2522 - accuracy: 0.9100 - val_loss: 0.5139 - val_accuracy: 0.8483 - lr: 5.7143e-04\n",
            "Epoch 157/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.2496 - accuracy: 0.9106 - val_loss: 0.5432 - val_accuracy: 0.8455 - lr: 1.0857e-04\n",
            "Epoch 158/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.2548 - accuracy: 0.9105 - val_loss: 0.5350 - val_accuracy: 0.8419 - lr: 5.1429e-04\n",
            "Epoch 159/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.2491 - accuracy: 0.9119 - val_loss: 0.5268 - val_accuracy: 0.8445 - lr: 9.7143e-05\n",
            "Epoch 160/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.2514 - accuracy: 0.9117 - val_loss: 0.5301 - val_accuracy: 0.8434 - lr: 4.5714e-04\n",
            "Epoch 161/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.2466 - accuracy: 0.9127 - val_loss: 0.5198 - val_accuracy: 0.8450 - lr: 8.5714e-05\n",
            "Epoch 162/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2477 - accuracy: 0.9138 - val_loss: 0.5220 - val_accuracy: 0.8468 - lr: 4.0000e-04\n",
            "Epoch 163/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.2408 - accuracy: 0.9151 - val_loss: 0.5293 - val_accuracy: 0.8453 - lr: 7.4286e-05\n",
            "Epoch 164/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.2447 - accuracy: 0.9129 - val_loss: 0.5344 - val_accuracy: 0.8431 - lr: 3.4286e-04\n",
            "Epoch 165/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2415 - accuracy: 0.9145 - val_loss: 0.5432 - val_accuracy: 0.8414 - lr: 6.2857e-05\n",
            "Epoch 166/175\n",
            "312/312 [==============================] - 22s 72ms/step - loss: 0.2365 - accuracy: 0.9157 - val_loss: 0.5312 - val_accuracy: 0.8456 - lr: 2.8571e-04\n",
            "Epoch 167/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2392 - accuracy: 0.9169 - val_loss: 0.5265 - val_accuracy: 0.8460 - lr: 5.1429e-05\n",
            "Epoch 168/175\n",
            "312/312 [==============================] - 23s 72ms/step - loss: 0.2375 - accuracy: 0.9181 - val_loss: 0.5198 - val_accuracy: 0.8468 - lr: 2.2857e-04\n",
            "Epoch 169/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.2412 - accuracy: 0.9144 - val_loss: 0.5295 - val_accuracy: 0.8454 - lr: 4.0000e-05\n",
            "Epoch 170/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2388 - accuracy: 0.9156 - val_loss: 0.5284 - val_accuracy: 0.8452 - lr: 1.7143e-04\n",
            "Epoch 171/175\n",
            "312/312 [==============================] - 22s 70ms/step - loss: 0.2348 - accuracy: 0.9182 - val_loss: 0.5230 - val_accuracy: 0.8469 - lr: 2.8571e-05\n",
            "Epoch 172/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2336 - accuracy: 0.9184 - val_loss: 0.5239 - val_accuracy: 0.8464 - lr: 1.1429e-04\n",
            "Epoch 173/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2374 - accuracy: 0.9165 - val_loss: 0.5223 - val_accuracy: 0.8479 - lr: 1.7143e-05\n",
            "Epoch 174/175\n",
            "312/312 [==============================] - 23s 73ms/step - loss: 0.2316 - accuracy: 0.9188 - val_loss: 0.5251 - val_accuracy: 0.8468 - lr: 5.7143e-05\n",
            "Epoch 175/175\n",
            "312/312 [==============================] - 22s 71ms/step - loss: 0.2385 - accuracy: 0.9167 - val_loss: 0.5221 - val_accuracy: 0.8479 - lr: 5.7143e-06\n",
            "Test loss: 0.5443543791770935\n",
            "Test accuracy: 0.8403000235557556\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, MaxPool2D, Flatten, Dense, Activation, BatchNormalization, Lambda, PReLU, LeakyReLU, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
        "from tensorflow.keras.layers import Flatten, Input, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\"\"\"\n",
        "The CIFAR10 dataset was downloaded through the official website, with each training batch being unpickled and then appended to each other\n",
        "to create one large training set. The images were preprocessed to convert the initial row vector to shape (32, 32, 3) through reshaping and transposing.\n",
        "The class output data was one hot encoded. My preliminary attempt used my model for the MNIST dataset, with an alteration for the input size. This\n",
        "resulted in a test accuracy of 67% after 10 epochs. My next attempt was a VGG with fractional max pooling, based on a paper by Benjamin Graham. While this definitely \n",
        "outperformed the previous model, the computational time was far too high. I tried reducing training time by using only one of the 5 training batches and doubling the \n",
        "batch size to 512, but the tradeoff with accuracy was way too high. \n",
        "\n",
        "The next model I tried was another VGG type convolutional network, which was shallower and converged much faster. This got me to 80% test accuracy.   \n",
        "\"\"\"\n",
        "# Fractional max pooling\n",
        "# - https://arxiv.org/abs/1412.6071\n",
        "# - https://github.com/laplacetw/vgg-like-cifar10/blob/master/fmp_cifar10.py\n",
        "# https://www.binarystudy.com/2021/09/how-to-load-preprocess-visualize-CIFAR-10-and-CIFAR-100.html#routine\n",
        "\n",
        "BTEST = '../data/test_batch'\n",
        "meta_file = '../CIFAR10-data/batches.meta'\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "NUM_TRAINING_BATCHES = 5\n",
        "BATCH_SIZE = 128 #128\n",
        "LAMBDA = 1e-5\n",
        "EPOCHS = 175\n",
        "IMG_SIDE_LEN = 32\n",
        "LR = 5e-3\n",
        "DATASET = \"CIFAR10\"\n",
        "MODEL = \"model3x\"\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        u = pickle._Unpickler( fo )\n",
        "        u.encoding = 'latin1'\n",
        "        dict = u.load()\n",
        "    return dict\n",
        "\n",
        "def load_training_data():\n",
        "    # The whole data_batch_1 has 10,000 images. And each image is a 1-D array having 3,072 entries. \n",
        "    # First 1024 entries for Red, the next 1024 entries for Green and last 1024 entries for Blue channels. \n",
        "    print(\"Loading Data:\")\n",
        "    features, classes = np.empty((0,32,32,3)), np.empty((0,10))\n",
        "    for i in range(NUM_TRAINING_BATCHES):\n",
        "        print(f\"Batch {i+1}\")\n",
        "        batch_path = f'../data/data_batch_{i+1}'\n",
        "        x, y = reshape_features(batch_path)\n",
        "        features = np.append(features, x, axis=0)\n",
        "        classes = np.append(classes, y, axis=0)\n",
        "    return features, classes\n",
        "\n",
        "def reshape_features(feat_path, CIFAR100=False):\n",
        "    labels = 'coarse_labels' if CIFAR100 else 'labels'\n",
        "    unpickled_data = unpickle(feat_path)\n",
        "    return (unpickled_data['data'].reshape(len(unpickled_data['data']),3,32,32).transpose(0,2,3,1) / 255,\n",
        "            tf.keras.utils.to_categorical(unpickled_data[labels]))\n",
        "\n",
        "def frac_max_pool(x):\n",
        "    return tf.nn.fractional_max_pool(x, [1.0, 1.41, 1.41, 1.0], pseudo_random=True, overlapping=True)[0]\n",
        "\n",
        "def poly_decay(epoch):\n",
        "  maxEpochs = EPOCHS\n",
        "  baseLR = LR\n",
        "  power = 1.0\n",
        "  alpha = baseLR * (1 - (epoch   / float(maxEpochs))) ** power\n",
        "  return alpha\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    horizontal_flip=True,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        "    #zoom_range=0.3\n",
        ")\n",
        "\n",
        "aug = ImageDataGenerator(width_shift_range=0.1,height_shift_range=0.1, horizontal_flip=True,fill_mode=\"nearest\")\n",
        "\n",
        "\n",
        "def normalize_x_data(x_train, x_test):\n",
        "    eps = 1e-7\n",
        "    mean = np.mean(x_train,axis = (0, 1, 2, 3))\n",
        "    std = np.std(x_train,axis = (0, 1, 2, 3))\n",
        "    x_train = (x_train - mean)/(std + eps)\n",
        "    x_test = (x_test - mean)/(std + eps)\n",
        "    return x_train, x_test\n",
        "\n",
        "class Data10(object):\n",
        "    def __init__(self):\n",
        "        self.x_train, self.y_train = load_training_data()\n",
        "        self.x_test, self.y_test = reshape_features(BTEST)\n",
        "        self.x_train, self.x_test = normalize_x_data(self.x_train, self.x_test)\n",
        "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train, self.y_train, test_size=0.2, random_state=31415)\n",
        "\n",
        "class Data100(object):\n",
        "    def __init__(self):\n",
        "        self.x_train, self.y_train = reshape_features('../CIFAR100-data/train', CIFAR100=True)\n",
        "        self.x_test, self.y_test = reshape_features('../CIFAR100-data/test', CIFAR100=True)\n",
        "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train, self.y_train, test_size=0.2, random_state=31415)\n",
        "\n",
        "def double_conv_module(input, num_filters, activation, kern_reg, dropout, padding=\"same\"):\n",
        "    input = Conv2D(filters = num_filters, kernel_size = (3, 3), activation = activation, padding = padding, kernel_regularizer = kern_reg)(input)\n",
        "    input = BatchNormalization(axis=-1)(input)\n",
        "    input = Conv2D(filters = num_filters, kernel_size = (3, 3), activation = activation, padding = padding, kernel_regularizer = kern_reg)(input)\n",
        "    input = BatchNormalization(axis=-1)(input)\n",
        "    input = MaxPooling2D(pool_size = (2, 2))(input)\n",
        "    input = Dropout(dropout)(input)\n",
        "\n",
        "    return input\n",
        "\n",
        "def rav_model(width, height, depth, classes):\n",
        "    inputShape=(height, width, depth)\n",
        "    weight_decay = 0.001\n",
        "\n",
        "    # (Step 1) Define the model input\n",
        "    inputs = Input(shape=inputShape)\n",
        "    KR = None #l2(weight_decay)\n",
        "    x = double_conv_module(inputs, 32, activation='relu', kern_reg=KR, dropout = 0.1, padding='same')\n",
        "    x = double_conv_module(inputs, 64, activation='relu', kern_reg=KR, dropout = 0.2, padding='same')\n",
        "    x = double_conv_module(inputs, 128, activation='relu', kern_reg=KR, dropout = 0.3, padding='same')\n",
        "    x = double_conv_module(inputs, 128, activation='relu', kern_reg=KR, dropout = 0.4, padding='same')\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu',kernel_regularizer=None)(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(classes)(x)\n",
        "    x = Activation(\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, x, name=\"rav_net\")\n",
        "    return model\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    if DATASET == 'CIFAR10':\n",
        "        data = Data10()\n",
        "    else:\n",
        "        data = Data100()\n",
        "\n",
        "    x_train, y_train = data.x_train, data.y_train\n",
        "    x_test, y_test = data.x_test, data.y_test\n",
        "    x_val, y_val = data.x_val, data.y_val\n",
        "\n",
        "    lr_scheduler = LearningRateScheduler(poly_decay)\n",
        "    variable_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor = 0.2, patience = 2)\n",
        "    model3x = rav_model(width=32, height=32, depth=3, classes=10)\n",
        "\n",
        "    ac='relu'\n",
        "    adm=Adam(learning_rate=0.001,decay=0, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "    opt=adm\n",
        "\n",
        "    model3x.compile(loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'],optimizer=opt)\n",
        "    model3x.summary()\n",
        "    history=model3x.fit(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE), \n",
        "                        batch_size=BATCH_SIZE, \n",
        "                        epochs=EPOCHS, \n",
        "                        callbacks=[variable_learning_rate, lr_scheduler], \n",
        "                        validation_data=(x_val, y_val), \n",
        "                        verbose=1, \n",
        "                        steps_per_epoch = len(x_train) // BATCH_SIZE)\n",
        "    score = model3x.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    print('Test loss:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "\n",
        "    # 30 epochs 82%"
      ]
    }
  ]
}