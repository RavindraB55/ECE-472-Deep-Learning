{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nia9qnBerH12",
        "outputId": "ac1ee691-7498-417e-d218-7f8a0918d5b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"rav_net\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d_58 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization_62 (Bat  (None, 32, 32, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_59 (Conv2D)          (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_63 (Bat  (None, 32, 32, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPoolin  (None, 16, 16, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_60 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_64 (Bat  (None, 16, 16, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_61 (Conv2D)          (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_65 (Bat  (None, 16, 16, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPoolin  (None, 8, 8, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_62 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_66 (Bat  (None, 8, 8, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_63 (Conv2D)          (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_67 (Bat  (None, 8, 8, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPoolin  (None, 4, 4, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " conv2d_64 (Conv2D)          (None, 4, 4, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_68 (Bat  (None, 4, 4, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_65 (Conv2D)          (None, 4, 4, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_69 (Bat  (None, 4, 4, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPoolin  (None, 2, 2, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 2, 2, 128)         0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " batch_normalization_70 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 20)                10260     \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 20)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 859,956\n",
            "Trainable params: 857,524\n",
            "Non-trainable params: 2,432\n",
            "_________________________________________________________________\n",
            "Epoch 1/150\n",
            "312/312 [==============================] - 22s 66ms/step - loss: 2.8157 - accuracy: 0.1991 - top_k_categorical_accuracy: 0.5637 - val_loss: 3.0057 - val_accuracy: 0.1610 - val_top_k_categorical_accuracy: 0.4487 - lr: 0.0050\n",
            "Epoch 2/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 2.3920 - accuracy: 0.2866 - top_k_categorical_accuracy: 0.6827 - val_loss: 2.5582 - val_accuracy: 0.2640 - val_top_k_categorical_accuracy: 0.6690 - lr: 0.0050\n",
            "Epoch 3/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 2.1699 - accuracy: 0.3420 - top_k_categorical_accuracy: 0.7395 - val_loss: 2.1478 - val_accuracy: 0.3465 - val_top_k_categorical_accuracy: 0.7186 - lr: 0.0049\n",
            "Epoch 4/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 1.9314 - accuracy: 0.3985 - top_k_categorical_accuracy: 0.7841 - val_loss: 2.2371 - val_accuracy: 0.3463 - val_top_k_categorical_accuracy: 0.7170 - lr: 0.0049\n",
            "Epoch 5/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 1.7479 - accuracy: 0.4514 - top_k_categorical_accuracy: 0.8224 - val_loss: 1.7473 - val_accuracy: 0.4550 - val_top_k_categorical_accuracy: 0.8290 - lr: 0.0049\n",
            "Epoch 6/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 1.6239 - accuracy: 0.4887 - top_k_categorical_accuracy: 0.8471 - val_loss: 1.7105 - val_accuracy: 0.4761 - val_top_k_categorical_accuracy: 0.8119 - lr: 0.0048\n",
            "Epoch 7/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 1.5073 - accuracy: 0.5238 - top_k_categorical_accuracy: 0.8697 - val_loss: 1.5175 - val_accuracy: 0.5312 - val_top_k_categorical_accuracy: 0.8698 - lr: 0.0048\n",
            "Epoch 8/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 1.4121 - accuracy: 0.5519 - top_k_categorical_accuracy: 0.8861 - val_loss: 1.6256 - val_accuracy: 0.5245 - val_top_k_categorical_accuracy: 0.8594 - lr: 0.0048\n",
            "Epoch 9/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 1.3431 - accuracy: 0.5741 - top_k_categorical_accuracy: 0.8969 - val_loss: 1.4668 - val_accuracy: 0.5559 - val_top_k_categorical_accuracy: 0.8864 - lr: 0.0047\n",
            "Epoch 10/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 1.2772 - accuracy: 0.5935 - top_k_categorical_accuracy: 0.9075 - val_loss: 1.7399 - val_accuracy: 0.4966 - val_top_k_categorical_accuracy: 0.8376 - lr: 0.0047\n",
            "Epoch 11/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 1.2172 - accuracy: 0.6119 - top_k_categorical_accuracy: 0.9148 - val_loss: 1.6040 - val_accuracy: 0.5282 - val_top_k_categorical_accuracy: 0.8694 - lr: 9.3333e-04\n",
            "Epoch 12/150\n",
            "312/312 [==============================] - 19s 62ms/step - loss: 1.1710 - accuracy: 0.6258 - top_k_categorical_accuracy: 0.9204 - val_loss: 1.2730 - val_accuracy: 0.6135 - val_top_k_categorical_accuracy: 0.9091 - lr: 0.0046\n",
            "Epoch 13/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 1.1247 - accuracy: 0.6391 - top_k_categorical_accuracy: 0.9272 - val_loss: 1.2998 - val_accuracy: 0.6028 - val_top_k_categorical_accuracy: 0.9118 - lr: 0.0046\n",
            "Epoch 14/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 1.0870 - accuracy: 0.6545 - top_k_categorical_accuracy: 0.9315 - val_loss: 1.3779 - val_accuracy: 0.5944 - val_top_k_categorical_accuracy: 0.8878 - lr: 9.1333e-04\n",
            "Epoch 15/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 1.0517 - accuracy: 0.6647 - top_k_categorical_accuracy: 0.9351 - val_loss: 1.2488 - val_accuracy: 0.6270 - val_top_k_categorical_accuracy: 0.9151 - lr: 0.0045\n",
            "Epoch 16/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 1.0180 - accuracy: 0.6742 - top_k_categorical_accuracy: 0.9389 - val_loss: 1.2801 - val_accuracy: 0.6180 - val_top_k_categorical_accuracy: 0.9085 - lr: 0.0045\n",
            "Epoch 17/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.9886 - accuracy: 0.6837 - top_k_categorical_accuracy: 0.9415 - val_loss: 1.2519 - val_accuracy: 0.6261 - val_top_k_categorical_accuracy: 0.9078 - lr: 8.9333e-04\n",
            "Epoch 18/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.9554 - accuracy: 0.6923 - top_k_categorical_accuracy: 0.9464 - val_loss: 1.1504 - val_accuracy: 0.6521 - val_top_k_categorical_accuracy: 0.9248 - lr: 0.0044\n",
            "Epoch 19/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.9189 - accuracy: 0.7002 - top_k_categorical_accuracy: 0.9513 - val_loss: 1.2280 - val_accuracy: 0.6336 - val_top_k_categorical_accuracy: 0.9130 - lr: 0.0044\n",
            "Epoch 20/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.9030 - accuracy: 0.7063 - top_k_categorical_accuracy: 0.9531 - val_loss: 1.1342 - val_accuracy: 0.6548 - val_top_k_categorical_accuracy: 0.9239 - lr: 0.0044\n",
            "Epoch 21/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.8751 - accuracy: 0.7191 - top_k_categorical_accuracy: 0.9545 - val_loss: 1.2024 - val_accuracy: 0.6426 - val_top_k_categorical_accuracy: 0.9187 - lr: 0.0043\n",
            "Epoch 22/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.8538 - accuracy: 0.7243 - top_k_categorical_accuracy: 0.9566 - val_loss: 1.1254 - val_accuracy: 0.6647 - val_top_k_categorical_accuracy: 0.9261 - lr: 0.0043\n",
            "Epoch 23/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.8333 - accuracy: 0.7315 - top_k_categorical_accuracy: 0.9589 - val_loss: 1.1561 - val_accuracy: 0.6525 - val_top_k_categorical_accuracy: 0.9196 - lr: 0.0043\n",
            "Epoch 24/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.8115 - accuracy: 0.7386 - top_k_categorical_accuracy: 0.9607 - val_loss: 1.2095 - val_accuracy: 0.6488 - val_top_k_categorical_accuracy: 0.9208 - lr: 8.4667e-04\n",
            "Epoch 25/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.7888 - accuracy: 0.7427 - top_k_categorical_accuracy: 0.9627 - val_loss: 1.1467 - val_accuracy: 0.6606 - val_top_k_categorical_accuracy: 0.9246 - lr: 0.0042\n",
            "Epoch 26/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.7685 - accuracy: 0.7522 - top_k_categorical_accuracy: 0.9644 - val_loss: 1.1672 - val_accuracy: 0.6640 - val_top_k_categorical_accuracy: 0.9245 - lr: 8.3333e-04\n",
            "Epoch 27/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.7566 - accuracy: 0.7545 - top_k_categorical_accuracy: 0.9663 - val_loss: 1.1035 - val_accuracy: 0.6732 - val_top_k_categorical_accuracy: 0.9308 - lr: 0.0041\n",
            "Epoch 28/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.7329 - accuracy: 0.7625 - top_k_categorical_accuracy: 0.9680 - val_loss: 1.1765 - val_accuracy: 0.6591 - val_top_k_categorical_accuracy: 0.9259 - lr: 0.0041\n",
            "Epoch 29/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.7204 - accuracy: 0.7667 - top_k_categorical_accuracy: 0.9693 - val_loss: 1.1410 - val_accuracy: 0.6652 - val_top_k_categorical_accuracy: 0.9271 - lr: 8.1333e-04\n",
            "Epoch 30/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.6995 - accuracy: 0.7718 - top_k_categorical_accuracy: 0.9714 - val_loss: 1.3546 - val_accuracy: 0.6271 - val_top_k_categorical_accuracy: 0.9026 - lr: 0.0040\n",
            "Epoch 31/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.6901 - accuracy: 0.7771 - top_k_categorical_accuracy: 0.9712 - val_loss: 1.1790 - val_accuracy: 0.6599 - val_top_k_categorical_accuracy: 0.9248 - lr: 8.0000e-04\n",
            "Epoch 32/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.6751 - accuracy: 0.7810 - top_k_categorical_accuracy: 0.9735 - val_loss: 1.1890 - val_accuracy: 0.6653 - val_top_k_categorical_accuracy: 0.9236 - lr: 0.0040\n",
            "Epoch 33/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.6564 - accuracy: 0.7870 - top_k_categorical_accuracy: 0.9743 - val_loss: 1.1941 - val_accuracy: 0.6582 - val_top_k_categorical_accuracy: 0.9203 - lr: 7.8667e-04\n",
            "Epoch 34/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.6464 - accuracy: 0.7898 - top_k_categorical_accuracy: 0.9745 - val_loss: 1.1900 - val_accuracy: 0.6644 - val_top_k_categorical_accuracy: 0.9226 - lr: 0.0039\n",
            "Epoch 35/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.6288 - accuracy: 0.7956 - top_k_categorical_accuracy: 0.9768 - val_loss: 1.0943 - val_accuracy: 0.6864 - val_top_k_categorical_accuracy: 0.9342 - lr: 0.0039\n",
            "Epoch 36/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.6177 - accuracy: 0.7994 - top_k_categorical_accuracy: 0.9770 - val_loss: 1.2281 - val_accuracy: 0.6561 - val_top_k_categorical_accuracy: 0.9220 - lr: 0.0038\n",
            "Epoch 37/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.6046 - accuracy: 0.8017 - top_k_categorical_accuracy: 0.9770 - val_loss: 1.0424 - val_accuracy: 0.6947 - val_top_k_categorical_accuracy: 0.9369 - lr: 0.0038\n",
            "Epoch 38/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.5909 - accuracy: 0.8071 - top_k_categorical_accuracy: 0.9795 - val_loss: 1.2267 - val_accuracy: 0.6648 - val_top_k_categorical_accuracy: 0.9260 - lr: 0.0038\n",
            "Epoch 39/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.5808 - accuracy: 0.8101 - top_k_categorical_accuracy: 0.9800 - val_loss: 1.1264 - val_accuracy: 0.6824 - val_top_k_categorical_accuracy: 0.9320 - lr: 7.4667e-04\n",
            "Epoch 40/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.5672 - accuracy: 0.8147 - top_k_categorical_accuracy: 0.9805 - val_loss: 1.1143 - val_accuracy: 0.6885 - val_top_k_categorical_accuracy: 0.9352 - lr: 0.0037\n",
            "Epoch 41/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.5576 - accuracy: 0.8160 - top_k_categorical_accuracy: 0.9816 - val_loss: 1.1965 - val_accuracy: 0.6823 - val_top_k_categorical_accuracy: 0.9289 - lr: 7.3333e-04\n",
            "Epoch 42/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.5449 - accuracy: 0.8206 - top_k_categorical_accuracy: 0.9823 - val_loss: 1.2225 - val_accuracy: 0.6826 - val_top_k_categorical_accuracy: 0.9296 - lr: 0.0036\n",
            "Epoch 43/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.5357 - accuracy: 0.8242 - top_k_categorical_accuracy: 0.9838 - val_loss: 1.1331 - val_accuracy: 0.6950 - val_top_k_categorical_accuracy: 0.9322 - lr: 7.2000e-04\n",
            "Epoch 44/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.5278 - accuracy: 0.8261 - top_k_categorical_accuracy: 0.9838 - val_loss: 1.1182 - val_accuracy: 0.6988 - val_top_k_categorical_accuracy: 0.9353 - lr: 0.0036\n",
            "Epoch 45/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.5194 - accuracy: 0.8260 - top_k_categorical_accuracy: 0.9838 - val_loss: 1.1633 - val_accuracy: 0.6858 - val_top_k_categorical_accuracy: 0.9311 - lr: 7.0667e-04\n",
            "Epoch 46/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.5053 - accuracy: 0.8334 - top_k_categorical_accuracy: 0.9839 - val_loss: 1.1491 - val_accuracy: 0.6865 - val_top_k_categorical_accuracy: 0.9325 - lr: 0.0035\n",
            "Epoch 47/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4958 - accuracy: 0.8363 - top_k_categorical_accuracy: 0.9864 - val_loss: 1.2151 - val_accuracy: 0.6781 - val_top_k_categorical_accuracy: 0.9267 - lr: 6.9333e-04\n",
            "Epoch 48/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.4913 - accuracy: 0.8377 - top_k_categorical_accuracy: 0.9857 - val_loss: 1.0695 - val_accuracy: 0.7071 - val_top_k_categorical_accuracy: 0.9406 - lr: 0.0034\n",
            "Epoch 49/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.4817 - accuracy: 0.8392 - top_k_categorical_accuracy: 0.9864 - val_loss: 1.1178 - val_accuracy: 0.6983 - val_top_k_categorical_accuracy: 0.9382 - lr: 6.8000e-04\n",
            "Epoch 50/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4720 - accuracy: 0.8460 - top_k_categorical_accuracy: 0.9874 - val_loss: 1.2635 - val_accuracy: 0.6761 - val_top_k_categorical_accuracy: 0.9262 - lr: 0.0034\n",
            "Epoch 51/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4600 - accuracy: 0.8474 - top_k_categorical_accuracy: 0.9884 - val_loss: 1.2316 - val_accuracy: 0.6899 - val_top_k_categorical_accuracy: 0.9274 - lr: 6.6667e-04\n",
            "Epoch 52/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.4585 - accuracy: 0.8480 - top_k_categorical_accuracy: 0.9878 - val_loss: 1.1763 - val_accuracy: 0.6926 - val_top_k_categorical_accuracy: 0.9295 - lr: 0.0033\n",
            "Epoch 53/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.4497 - accuracy: 0.8523 - top_k_categorical_accuracy: 0.9888 - val_loss: 1.2844 - val_accuracy: 0.6744 - val_top_k_categorical_accuracy: 0.9188 - lr: 6.5333e-04\n",
            "Epoch 54/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.4418 - accuracy: 0.8541 - top_k_categorical_accuracy: 0.9894 - val_loss: 1.1177 - val_accuracy: 0.7042 - val_top_k_categorical_accuracy: 0.9380 - lr: 0.0032\n",
            "Epoch 55/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4313 - accuracy: 0.8565 - top_k_categorical_accuracy: 0.9896 - val_loss: 1.1904 - val_accuracy: 0.6926 - val_top_k_categorical_accuracy: 0.9359 - lr: 6.4000e-04\n",
            "Epoch 56/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4245 - accuracy: 0.8567 - top_k_categorical_accuracy: 0.9914 - val_loss: 1.1537 - val_accuracy: 0.7065 - val_top_k_categorical_accuracy: 0.9385 - lr: 0.0032\n",
            "Epoch 57/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4234 - accuracy: 0.8599 - top_k_categorical_accuracy: 0.9889 - val_loss: 1.2359 - val_accuracy: 0.6887 - val_top_k_categorical_accuracy: 0.9360 - lr: 6.2667e-04\n",
            "Epoch 58/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4054 - accuracy: 0.8653 - top_k_categorical_accuracy: 0.9915 - val_loss: 1.1947 - val_accuracy: 0.6947 - val_top_k_categorical_accuracy: 0.9340 - lr: 0.0031\n",
            "Epoch 59/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4043 - accuracy: 0.8651 - top_k_categorical_accuracy: 0.9907 - val_loss: 1.1765 - val_accuracy: 0.6961 - val_top_k_categorical_accuracy: 0.9376 - lr: 6.1333e-04\n",
            "Epoch 60/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3902 - accuracy: 0.8696 - top_k_categorical_accuracy: 0.9919 - val_loss: 1.2183 - val_accuracy: 0.7006 - val_top_k_categorical_accuracy: 0.9344 - lr: 0.0030\n",
            "Epoch 61/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3898 - accuracy: 0.8715 - top_k_categorical_accuracy: 0.9919 - val_loss: 1.2600 - val_accuracy: 0.6862 - val_top_k_categorical_accuracy: 0.9317 - lr: 6.0000e-04\n",
            "Epoch 62/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3824 - accuracy: 0.8721 - top_k_categorical_accuracy: 0.9923 - val_loss: 1.1723 - val_accuracy: 0.7072 - val_top_k_categorical_accuracy: 0.9397 - lr: 0.0030\n",
            "Epoch 63/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3847 - accuracy: 0.8697 - top_k_categorical_accuracy: 0.9917 - val_loss: 1.1548 - val_accuracy: 0.7129 - val_top_k_categorical_accuracy: 0.9428 - lr: 5.8667e-04\n",
            "Epoch 64/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.3722 - accuracy: 0.8753 - top_k_categorical_accuracy: 0.9922 - val_loss: 1.1865 - val_accuracy: 0.7046 - val_top_k_categorical_accuracy: 0.9359 - lr: 0.0029\n",
            "Epoch 65/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3669 - accuracy: 0.8764 - top_k_categorical_accuracy: 0.9928 - val_loss: 1.1952 - val_accuracy: 0.7019 - val_top_k_categorical_accuracy: 0.9366 - lr: 5.7333e-04\n",
            "Epoch 66/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3645 - accuracy: 0.8773 - top_k_categorical_accuracy: 0.9928 - val_loss: 1.2681 - val_accuracy: 0.6873 - val_top_k_categorical_accuracy: 0.9336 - lr: 0.0028\n",
            "Epoch 67/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.3495 - accuracy: 0.8834 - top_k_categorical_accuracy: 0.9932 - val_loss: 1.2601 - val_accuracy: 0.6989 - val_top_k_categorical_accuracy: 0.9353 - lr: 5.6000e-04\n",
            "Epoch 68/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3461 - accuracy: 0.8841 - top_k_categorical_accuracy: 0.9940 - val_loss: 1.2177 - val_accuracy: 0.7112 - val_top_k_categorical_accuracy: 0.9370 - lr: 0.0028\n",
            "Epoch 69/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3406 - accuracy: 0.8867 - top_k_categorical_accuracy: 0.9940 - val_loss: 1.2139 - val_accuracy: 0.7048 - val_top_k_categorical_accuracy: 0.9385 - lr: 5.4667e-04\n",
            "Epoch 70/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3376 - accuracy: 0.8869 - top_k_categorical_accuracy: 0.9941 - val_loss: 1.2104 - val_accuracy: 0.7044 - val_top_k_categorical_accuracy: 0.9371 - lr: 0.0027\n",
            "Epoch 71/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3305 - accuracy: 0.8885 - top_k_categorical_accuracy: 0.9941 - val_loss: 1.2184 - val_accuracy: 0.7113 - val_top_k_categorical_accuracy: 0.9392 - lr: 5.3333e-04\n",
            "Epoch 72/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3313 - accuracy: 0.8893 - top_k_categorical_accuracy: 0.9943 - val_loss: 1.2185 - val_accuracy: 0.7107 - val_top_k_categorical_accuracy: 0.9394 - lr: 0.0026\n",
            "Epoch 73/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3125 - accuracy: 0.8961 - top_k_categorical_accuracy: 0.9946 - val_loss: 1.2938 - val_accuracy: 0.6962 - val_top_k_categorical_accuracy: 0.9371 - lr: 5.2000e-04\n",
            "Epoch 74/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.3189 - accuracy: 0.8932 - top_k_categorical_accuracy: 0.9946 - val_loss: 1.3000 - val_accuracy: 0.6935 - val_top_k_categorical_accuracy: 0.9348 - lr: 0.0026\n",
            "Epoch 75/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.3135 - accuracy: 0.8937 - top_k_categorical_accuracy: 0.9948 - val_loss: 1.3688 - val_accuracy: 0.6904 - val_top_k_categorical_accuracy: 0.9319 - lr: 5.0667e-04\n",
            "Epoch 76/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3078 - accuracy: 0.8966 - top_k_categorical_accuracy: 0.9949 - val_loss: 1.2492 - val_accuracy: 0.7111 - val_top_k_categorical_accuracy: 0.9362 - lr: 0.0025\n",
            "Epoch 77/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3066 - accuracy: 0.8964 - top_k_categorical_accuracy: 0.9953 - val_loss: 1.2375 - val_accuracy: 0.7094 - val_top_k_categorical_accuracy: 0.9374 - lr: 4.9333e-04\n",
            "Epoch 78/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2973 - accuracy: 0.8997 - top_k_categorical_accuracy: 0.9953 - val_loss: 1.2839 - val_accuracy: 0.7024 - val_top_k_categorical_accuracy: 0.9360 - lr: 0.0024\n",
            "Epoch 79/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2899 - accuracy: 0.9023 - top_k_categorical_accuracy: 0.9959 - val_loss: 1.3180 - val_accuracy: 0.7034 - val_top_k_categorical_accuracy: 0.9384 - lr: 4.8000e-04\n",
            "Epoch 80/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2912 - accuracy: 0.9028 - top_k_categorical_accuracy: 0.9957 - val_loss: 1.2912 - val_accuracy: 0.7012 - val_top_k_categorical_accuracy: 0.9310 - lr: 0.0024\n",
            "Epoch 81/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2852 - accuracy: 0.9037 - top_k_categorical_accuracy: 0.9953 - val_loss: 1.3726 - val_accuracy: 0.6913 - val_top_k_categorical_accuracy: 0.9342 - lr: 4.6667e-04\n",
            "Epoch 82/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2773 - accuracy: 0.9060 - top_k_categorical_accuracy: 0.9963 - val_loss: 1.3166 - val_accuracy: 0.7046 - val_top_k_categorical_accuracy: 0.9347 - lr: 0.0023\n",
            "Epoch 83/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2775 - accuracy: 0.9079 - top_k_categorical_accuracy: 0.9958 - val_loss: 1.2633 - val_accuracy: 0.7113 - val_top_k_categorical_accuracy: 0.9361 - lr: 4.5333e-04\n",
            "Epoch 84/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2693 - accuracy: 0.9094 - top_k_categorical_accuracy: 0.9963 - val_loss: 1.2984 - val_accuracy: 0.7114 - val_top_k_categorical_accuracy: 0.9369 - lr: 0.0022\n",
            "Epoch 85/150\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.2648 - accuracy: 0.9123 - top_k_categorical_accuracy: 0.9965 - val_loss: 1.2295 - val_accuracy: 0.7255 - val_top_k_categorical_accuracy: 0.9443 - lr: 4.4000e-04\n",
            "Epoch 86/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2659 - accuracy: 0.9116 - top_k_categorical_accuracy: 0.9961 - val_loss: 1.2464 - val_accuracy: 0.7145 - val_top_k_categorical_accuracy: 0.9397 - lr: 0.0022\n",
            "Epoch 87/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2593 - accuracy: 0.9140 - top_k_categorical_accuracy: 0.9961 - val_loss: 1.2776 - val_accuracy: 0.7111 - val_top_k_categorical_accuracy: 0.9365 - lr: 4.2667e-04\n",
            "Epoch 88/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2506 - accuracy: 0.9155 - top_k_categorical_accuracy: 0.9969 - val_loss: 1.2704 - val_accuracy: 0.7164 - val_top_k_categorical_accuracy: 0.9395 - lr: 0.0021\n",
            "Epoch 89/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2479 - accuracy: 0.9161 - top_k_categorical_accuracy: 0.9967 - val_loss: 1.3056 - val_accuracy: 0.7106 - val_top_k_categorical_accuracy: 0.9387 - lr: 4.1333e-04\n",
            "Epoch 90/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2496 - accuracy: 0.9152 - top_k_categorical_accuracy: 0.9968 - val_loss: 1.2900 - val_accuracy: 0.7063 - val_top_k_categorical_accuracy: 0.9370 - lr: 0.0020\n",
            "Epoch 91/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2570 - accuracy: 0.9132 - top_k_categorical_accuracy: 0.9966 - val_loss: 1.3249 - val_accuracy: 0.7014 - val_top_k_categorical_accuracy: 0.9350 - lr: 4.0000e-04\n",
            "Epoch 92/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2441 - accuracy: 0.9175 - top_k_categorical_accuracy: 0.9968 - val_loss: 1.3062 - val_accuracy: 0.7131 - val_top_k_categorical_accuracy: 0.9397 - lr: 0.0020\n",
            "Epoch 93/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2421 - accuracy: 0.9199 - top_k_categorical_accuracy: 0.9971 - val_loss: 1.2905 - val_accuracy: 0.7166 - val_top_k_categorical_accuracy: 0.9426 - lr: 3.8667e-04\n",
            "Epoch 94/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2331 - accuracy: 0.9198 - top_k_categorical_accuracy: 0.9978 - val_loss: 1.3062 - val_accuracy: 0.7167 - val_top_k_categorical_accuracy: 0.9396 - lr: 0.0019\n",
            "Epoch 95/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2228 - accuracy: 0.9245 - top_k_categorical_accuracy: 0.9974 - val_loss: 1.3057 - val_accuracy: 0.7218 - val_top_k_categorical_accuracy: 0.9423 - lr: 3.7333e-04\n",
            "Epoch 96/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2232 - accuracy: 0.9248 - top_k_categorical_accuracy: 0.9975 - val_loss: 1.3288 - val_accuracy: 0.7164 - val_top_k_categorical_accuracy: 0.9417 - lr: 0.0018\n",
            "Epoch 97/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2238 - accuracy: 0.9245 - top_k_categorical_accuracy: 0.9974 - val_loss: 1.3283 - val_accuracy: 0.7082 - val_top_k_categorical_accuracy: 0.9368 - lr: 3.6000e-04\n",
            "Epoch 98/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2186 - accuracy: 0.9267 - top_k_categorical_accuracy: 0.9978 - val_loss: 1.3005 - val_accuracy: 0.7164 - val_top_k_categorical_accuracy: 0.9421 - lr: 0.0018\n",
            "Epoch 99/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2156 - accuracy: 0.9275 - top_k_categorical_accuracy: 0.9975 - val_loss: 1.3204 - val_accuracy: 0.7176 - val_top_k_categorical_accuracy: 0.9408 - lr: 3.4667e-04\n",
            "Epoch 100/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2061 - accuracy: 0.9307 - top_k_categorical_accuracy: 0.9983 - val_loss: 1.4127 - val_accuracy: 0.7109 - val_top_k_categorical_accuracy: 0.9394 - lr: 0.0017\n",
            "Epoch 101/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2078 - accuracy: 0.9300 - top_k_categorical_accuracy: 0.9980 - val_loss: 1.3438 - val_accuracy: 0.7162 - val_top_k_categorical_accuracy: 0.9391 - lr: 3.3333e-04\n",
            "Epoch 102/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2101 - accuracy: 0.9286 - top_k_categorical_accuracy: 0.9977 - val_loss: 1.3721 - val_accuracy: 0.7109 - val_top_k_categorical_accuracy: 0.9397 - lr: 0.0016\n",
            "Epoch 103/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2062 - accuracy: 0.9300 - top_k_categorical_accuracy: 0.9978 - val_loss: 1.3416 - val_accuracy: 0.7177 - val_top_k_categorical_accuracy: 0.9395 - lr: 3.2000e-04\n",
            "Epoch 104/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2067 - accuracy: 0.9316 - top_k_categorical_accuracy: 0.9977 - val_loss: 1.4029 - val_accuracy: 0.7086 - val_top_k_categorical_accuracy: 0.9372 - lr: 0.0016\n",
            "Epoch 105/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1968 - accuracy: 0.9330 - top_k_categorical_accuracy: 0.9977 - val_loss: 1.3311 - val_accuracy: 0.7226 - val_top_k_categorical_accuracy: 0.9412 - lr: 3.0667e-04\n",
            "Epoch 106/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1929 - accuracy: 0.9341 - top_k_categorical_accuracy: 0.9981 - val_loss: 1.3583 - val_accuracy: 0.7197 - val_top_k_categorical_accuracy: 0.9402 - lr: 0.0015\n",
            "Epoch 107/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1948 - accuracy: 0.9352 - top_k_categorical_accuracy: 0.9985 - val_loss: 1.3388 - val_accuracy: 0.7212 - val_top_k_categorical_accuracy: 0.9395 - lr: 2.9333e-04\n",
            "Epoch 108/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1949 - accuracy: 0.9341 - top_k_categorical_accuracy: 0.9982 - val_loss: 1.3852 - val_accuracy: 0.7150 - val_top_k_categorical_accuracy: 0.9374 - lr: 0.0014\n",
            "Epoch 109/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1955 - accuracy: 0.9329 - top_k_categorical_accuracy: 0.9983 - val_loss: 1.3546 - val_accuracy: 0.7187 - val_top_k_categorical_accuracy: 0.9402 - lr: 2.8000e-04\n",
            "Epoch 110/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1840 - accuracy: 0.9378 - top_k_categorical_accuracy: 0.9983 - val_loss: 1.3995 - val_accuracy: 0.7138 - val_top_k_categorical_accuracy: 0.9388 - lr: 0.0014\n",
            "Epoch 111/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1758 - accuracy: 0.9388 - top_k_categorical_accuracy: 0.9986 - val_loss: 1.3723 - val_accuracy: 0.7187 - val_top_k_categorical_accuracy: 0.9423 - lr: 2.6667e-04\n",
            "Epoch 112/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1736 - accuracy: 0.9413 - top_k_categorical_accuracy: 0.9986 - val_loss: 1.3731 - val_accuracy: 0.7247 - val_top_k_categorical_accuracy: 0.9417 - lr: 0.0013\n",
            "Epoch 113/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1722 - accuracy: 0.9421 - top_k_categorical_accuracy: 0.9983 - val_loss: 1.3914 - val_accuracy: 0.7234 - val_top_k_categorical_accuracy: 0.9416 - lr: 2.5333e-04\n",
            "Epoch 114/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1759 - accuracy: 0.9399 - top_k_categorical_accuracy: 0.9989 - val_loss: 1.3843 - val_accuracy: 0.7237 - val_top_k_categorical_accuracy: 0.9418 - lr: 0.0012\n",
            "Epoch 115/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1688 - accuracy: 0.9419 - top_k_categorical_accuracy: 0.9982 - val_loss: 1.3230 - val_accuracy: 0.7308 - val_top_k_categorical_accuracy: 0.9433 - lr: 2.4000e-04\n",
            "Epoch 116/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1710 - accuracy: 0.9417 - top_k_categorical_accuracy: 0.9986 - val_loss: 1.4263 - val_accuracy: 0.7144 - val_top_k_categorical_accuracy: 0.9375 - lr: 0.0012\n",
            "Epoch 117/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1630 - accuracy: 0.9455 - top_k_categorical_accuracy: 0.9989 - val_loss: 1.3821 - val_accuracy: 0.7233 - val_top_k_categorical_accuracy: 0.9416 - lr: 2.2667e-04\n",
            "Epoch 118/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1576 - accuracy: 0.9471 - top_k_categorical_accuracy: 0.9990 - val_loss: 1.3862 - val_accuracy: 0.7232 - val_top_k_categorical_accuracy: 0.9422 - lr: 0.0011\n",
            "Epoch 119/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1635 - accuracy: 0.9450 - top_k_categorical_accuracy: 0.9988 - val_loss: 1.4235 - val_accuracy: 0.7196 - val_top_k_categorical_accuracy: 0.9374 - lr: 2.1333e-04\n",
            "Epoch 120/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1531 - accuracy: 0.9487 - top_k_categorical_accuracy: 0.9989 - val_loss: 1.4148 - val_accuracy: 0.7244 - val_top_k_categorical_accuracy: 0.9395 - lr: 0.0010\n",
            "Epoch 121/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1535 - accuracy: 0.9483 - top_k_categorical_accuracy: 0.9988 - val_loss: 1.4516 - val_accuracy: 0.7169 - val_top_k_categorical_accuracy: 0.9389 - lr: 2.0000e-04\n",
            "Epoch 122/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1525 - accuracy: 0.9490 - top_k_categorical_accuracy: 0.9988 - val_loss: 1.4205 - val_accuracy: 0.7222 - val_top_k_categorical_accuracy: 0.9417 - lr: 9.6667e-04\n",
            "Epoch 123/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1492 - accuracy: 0.9497 - top_k_categorical_accuracy: 0.9985 - val_loss: 1.4261 - val_accuracy: 0.7219 - val_top_k_categorical_accuracy: 0.9414 - lr: 1.8667e-04\n",
            "Epoch 124/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1408 - accuracy: 0.9525 - top_k_categorical_accuracy: 0.9989 - val_loss: 1.4148 - val_accuracy: 0.7224 - val_top_k_categorical_accuracy: 0.9426 - lr: 9.0000e-04\n",
            "Epoch 125/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1461 - accuracy: 0.9502 - top_k_categorical_accuracy: 0.9989 - val_loss: 1.4675 - val_accuracy: 0.7169 - val_top_k_categorical_accuracy: 0.9403 - lr: 1.7333e-04\n",
            "Epoch 126/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1457 - accuracy: 0.9510 - top_k_categorical_accuracy: 0.9991 - val_loss: 1.4464 - val_accuracy: 0.7171 - val_top_k_categorical_accuracy: 0.9407 - lr: 8.3333e-04\n",
            "Epoch 127/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1371 - accuracy: 0.9531 - top_k_categorical_accuracy: 0.9992 - val_loss: 1.4267 - val_accuracy: 0.7274 - val_top_k_categorical_accuracy: 0.9436 - lr: 1.6000e-04\n",
            "Epoch 128/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1333 - accuracy: 0.9539 - top_k_categorical_accuracy: 0.9991 - val_loss: 1.4413 - val_accuracy: 0.7265 - val_top_k_categorical_accuracy: 0.9436 - lr: 7.6667e-04\n",
            "Epoch 129/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1378 - accuracy: 0.9533 - top_k_categorical_accuracy: 0.9991 - val_loss: 1.4150 - val_accuracy: 0.7291 - val_top_k_categorical_accuracy: 0.9427 - lr: 1.4667e-04\n",
            "Epoch 130/150\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.1380 - accuracy: 0.9538 - top_k_categorical_accuracy: 0.9990 - val_loss: 1.4298 - val_accuracy: 0.7265 - val_top_k_categorical_accuracy: 0.9431 - lr: 7.0000e-04\n",
            "Epoch 131/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1287 - accuracy: 0.9569 - top_k_categorical_accuracy: 0.9992 - val_loss: 1.4207 - val_accuracy: 0.7312 - val_top_k_categorical_accuracy: 0.9458 - lr: 1.3333e-04\n",
            "Epoch 132/150\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1316 - accuracy: 0.9555 - top_k_categorical_accuracy: 0.9990 - val_loss: 1.4327 - val_accuracy: 0.7265 - val_top_k_categorical_accuracy: 0.9429 - lr: 6.3333e-04\n",
            "Epoch 133/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1261 - accuracy: 0.9575 - top_k_categorical_accuracy: 0.9992 - val_loss: 1.4369 - val_accuracy: 0.7302 - val_top_k_categorical_accuracy: 0.9430 - lr: 1.2000e-04\n",
            "Epoch 134/150\n",
            "312/312 [==============================] - 20s 66ms/step - loss: 0.1261 - accuracy: 0.9573 - top_k_categorical_accuracy: 0.9992 - val_loss: 1.4435 - val_accuracy: 0.7286 - val_top_k_categorical_accuracy: 0.9445 - lr: 5.6667e-04\n",
            "Epoch 135/150\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.1284 - accuracy: 0.9567 - top_k_categorical_accuracy: 0.9991 - val_loss: 1.4309 - val_accuracy: 0.7283 - val_top_k_categorical_accuracy: 0.9451 - lr: 1.0667e-04\n",
            "Epoch 136/150\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.1242 - accuracy: 0.9570 - top_k_categorical_accuracy: 0.9992 - val_loss: 1.4228 - val_accuracy: 0.7345 - val_top_k_categorical_accuracy: 0.9438 - lr: 5.0000e-04\n",
            "Epoch 137/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1216 - accuracy: 0.9589 - top_k_categorical_accuracy: 0.9994 - val_loss: 1.4561 - val_accuracy: 0.7256 - val_top_k_categorical_accuracy: 0.9445 - lr: 9.3333e-05\n",
            "Epoch 138/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1175 - accuracy: 0.9604 - top_k_categorical_accuracy: 0.9993 - val_loss: 1.4730 - val_accuracy: 0.7262 - val_top_k_categorical_accuracy: 0.9417 - lr: 4.3333e-04\n",
            "Epoch 139/150\n",
            "312/312 [==============================] - 20s 66ms/step - loss: 0.1161 - accuracy: 0.9601 - top_k_categorical_accuracy: 0.9993 - val_loss: 1.4518 - val_accuracy: 0.7287 - val_top_k_categorical_accuracy: 0.9447 - lr: 8.0000e-05\n",
            "Epoch 140/150\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.1158 - accuracy: 0.9612 - top_k_categorical_accuracy: 0.9994 - val_loss: 1.4794 - val_accuracy: 0.7255 - val_top_k_categorical_accuracy: 0.9441 - lr: 3.6667e-04\n",
            "Epoch 141/150\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1131 - accuracy: 0.9628 - top_k_categorical_accuracy: 0.9994 - val_loss: 1.4439 - val_accuracy: 0.7297 - val_top_k_categorical_accuracy: 0.9456 - lr: 6.6667e-05\n",
            "Epoch 142/150\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.1135 - accuracy: 0.9614 - top_k_categorical_accuracy: 0.9993 - val_loss: 1.4784 - val_accuracy: 0.7274 - val_top_k_categorical_accuracy: 0.9450 - lr: 3.0000e-04\n",
            "Epoch 143/150\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.1090 - accuracy: 0.9631 - top_k_categorical_accuracy: 0.9994 - val_loss: 1.4761 - val_accuracy: 0.7281 - val_top_k_categorical_accuracy: 0.9452 - lr: 5.3333e-05\n",
            "Epoch 144/150\n",
            "312/312 [==============================] - 20s 66ms/step - loss: 0.1058 - accuracy: 0.9639 - top_k_categorical_accuracy: 0.9994 - val_loss: 1.4960 - val_accuracy: 0.7260 - val_top_k_categorical_accuracy: 0.9453 - lr: 2.3333e-04\n",
            "Epoch 145/150\n",
            "312/312 [==============================] - 20s 66ms/step - loss: 0.1069 - accuracy: 0.9637 - top_k_categorical_accuracy: 0.9995 - val_loss: 1.4827 - val_accuracy: 0.7288 - val_top_k_categorical_accuracy: 0.9444 - lr: 4.0000e-05\n",
            "Epoch 146/150\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.1023 - accuracy: 0.9655 - top_k_categorical_accuracy: 0.9994 - val_loss: 1.4818 - val_accuracy: 0.7274 - val_top_k_categorical_accuracy: 0.9454 - lr: 1.6667e-04\n",
            "Epoch 147/150\n",
            "312/312 [==============================] - 21s 66ms/step - loss: 0.1033 - accuracy: 0.9656 - top_k_categorical_accuracy: 0.9997 - val_loss: 1.4739 - val_accuracy: 0.7295 - val_top_k_categorical_accuracy: 0.9452 - lr: 2.6667e-05\n",
            "Epoch 148/150\n",
            "312/312 [==============================] - 21s 67ms/step - loss: 0.1027 - accuracy: 0.9637 - top_k_categorical_accuracy: 0.9997 - val_loss: 1.4829 - val_accuracy: 0.7290 - val_top_k_categorical_accuracy: 0.9466 - lr: 1.0000e-04\n",
            "Epoch 149/150\n",
            "312/312 [==============================] - 21s 67ms/step - loss: 0.1061 - accuracy: 0.9639 - top_k_categorical_accuracy: 0.9995 - val_loss: 1.4735 - val_accuracy: 0.7297 - val_top_k_categorical_accuracy: 0.9460 - lr: 1.3333e-05\n",
            "Epoch 150/150\n",
            "312/312 [==============================] - 21s 67ms/step - loss: 0.1059 - accuracy: 0.9645 - top_k_categorical_accuracy: 0.9994 - val_loss: 1.4735 - val_accuracy: 0.7299 - val_top_k_categorical_accuracy: 0.9462 - lr: 3.3333e-05\n",
            "Test loss: 1.4616538286209106\n",
            "Test accuracy: 0.7339000105857849\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, MaxPool2D, Flatten, Dense, Activation, BatchNormalization, Lambda, PReLU, LeakyReLU, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
        "from tensorflow.keras.layers import Flatten, Input, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\"\"\"\n",
        "The CIFAR10 dataset was downloaded through the official website, with each training batch being unpickled and then appended to each other\n",
        "to create one large training set. The images were preprocessed to convert the initial row vector to shape (32, 32, 3) through reshaping and transposing.\n",
        "The class output data was one hot encoded. My preliminary attempt used my model for the MNIST dataset, with an alteration for the input size. This\n",
        "resulted in a test accuracy of 67% after 10 epochs. My next attempt was a VGG with fractional max pooling, based on a paper by Benjamin Graham. While this definitely \n",
        "outperformed the previous model, the computational time was far too high. I tried reducing training time by using only one of the 5 training batches and doubling the \n",
        "batch size to 512, but the tradeoff with accuracy was way too high. \n",
        "\n",
        "The next model I tried was another VGG type convolutional network, which was shallower and converged much faster. This got me to 80% test accuracy.   \n",
        "\"\"\"\n",
        "# Fractional max pooling\n",
        "# - https://arxiv.org/abs/1412.6071\n",
        "# - https://github.com/laplacetw/vgg-like-cifar10/blob/master/fmp_cifar10.py\n",
        "# https://www.binarystudy.com/2021/09/how-to-load-preprocess-visualize-CIFAR-10-and-CIFAR-100.html#routine\n",
        "\n",
        "BTEST = '../data/test_batch'\n",
        "meta_file = '../CIFAR10-data/batches.meta'\n",
        "\n",
        "NUM_TRAINING_BATCHES = 5\n",
        "BATCH_SIZE = 128 #128\n",
        "LAMBDA = 1e-5\n",
        "EPOCHS = 150\n",
        "IMG_SIDE_LEN = 32\n",
        "LR = 5e-3\n",
        "DATASET = \"CIFAR100\"\n",
        "MODEL = \"model3x\"\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        u = pickle._Unpickler( fo )\n",
        "        u.encoding = 'latin1'\n",
        "        dict = u.load()\n",
        "    return dict\n",
        "\n",
        "def load_training_data():\n",
        "    # The whole data_batch_1 has 10,000 images. And each image is a 1-D array having 3,072 entries. \n",
        "    # First 1024 entries for Red, the next 1024 entries for Green and last 1024 entries for Blue channels. \n",
        "    print(\"Loading Data:\")\n",
        "    features, classes = np.empty((0,32,32,3)), np.empty((0,10))\n",
        "    for i in range(NUM_TRAINING_BATCHES):\n",
        "        print(f\"Batch {i+1}\")\n",
        "        batch_path = f'../data/data_batch_{i+1}'\n",
        "        x, y = reshape_features(batch_path)\n",
        "        features = np.append(features, x, axis=0)\n",
        "        classes = np.append(classes, y, axis=0)\n",
        "    return features, classes\n",
        "\n",
        "def reshape_features(feat_path, CIFAR100=False):\n",
        "    labels = 'coarse_labels' if CIFAR100 else 'labels'\n",
        "    unpickled_data = unpickle(feat_path)\n",
        "    return (unpickled_data['data'].reshape(len(unpickled_data['data']),3,32,32).transpose(0,2,3,1) / 255,\n",
        "            tf.keras.utils.to_categorical(unpickled_data[labels]))\n",
        "\n",
        "def frac_max_pool(x):\n",
        "    return tf.nn.fractional_max_pool(x, [1.0, 1.41, 1.41, 1.0], pseudo_random=True, overlapping=True)[0]\n",
        "\n",
        "def poly_decay(epoch):\n",
        "  maxEpochs = EPOCHS\n",
        "  baseLR = LR\n",
        "  power = 1.0\n",
        "  alpha = baseLR * (1 - (epoch   / float(maxEpochs))) ** power\n",
        "  return alpha\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    horizontal_flip=True,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        "    #zoom_range=0.3\n",
        ")\n",
        "\n",
        "aug = ImageDataGenerator(width_shift_range=0.1,height_shift_range=0.1, horizontal_flip=True,fill_mode=\"nearest\")\n",
        "\n",
        "\n",
        "def normalize_x_data(x_train, x_test):\n",
        "    eps = 1e-7\n",
        "    mean = np.mean(x_train,axis = (0, 1, 2, 3))\n",
        "    std = np.std(x_train,axis = (0, 1, 2, 3))\n",
        "    x_train = (x_train - mean)/(std + eps)\n",
        "    x_test = (x_test - mean)/(std + eps)\n",
        "    return x_train, x_test\n",
        "\n",
        "class Data10(object):\n",
        "    def __init__(self):\n",
        "        self.x_train, self.y_train = load_training_data()\n",
        "        self.x_test, self.y_test = reshape_features(BTEST)\n",
        "        self.x_train, self.x_test = normalize_x_data(self.x_train, self.x_test)\n",
        "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train, self.y_train, test_size=0.2, random_state=31415)\n",
        "\n",
        "class Data100(object):\n",
        "    def __init__(self):\n",
        "      self.x_train, self.y_train = reshape_features('../data/train', CIFAR100=True)\n",
        "      self.x_test, self.y_test = reshape_features('../data/test', CIFAR100=True)\n",
        "      self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train, self.y_train, test_size=0.2, random_state=31415)\n",
        "\n",
        "def double_conv_module(input, num_filters, activation, kern_reg, dropout, padding=\"same\"):\n",
        "    input = Conv2D(filters = num_filters, kernel_size = (3, 3), activation = activation, padding = padding, kernel_regularizer = kern_reg)(input)\n",
        "    input = BatchNormalization(axis=-1)(input)\n",
        "    input = Conv2D(filters = num_filters, kernel_size = (3, 3), activation = activation, padding = padding, kernel_regularizer = kern_reg)(input)\n",
        "    input = BatchNormalization(axis=-1)(input)\n",
        "    input = MaxPooling2D(pool_size = (2, 2))(input)\n",
        "    input = Dropout(dropout)(input)\n",
        "\n",
        "    return input\n",
        "\n",
        "def rav_model(width, height, depth, classes):\n",
        "    inputShape=(height, width, depth)\n",
        "    weight_decay = 0.001\n",
        "\n",
        "    # (Step 1) Define the model input\n",
        "    inputs = Input(shape=inputShape)\n",
        "    KR = None #l2(weight_decay)\n",
        "    x = double_conv_module(inputs, 32, activation='relu', kern_reg=KR, dropout = 0.1, padding='same')\n",
        "    x = double_conv_module(x, 64, activation='relu', kern_reg=KR, dropout = 0.2, padding='same')\n",
        "    x = double_conv_module(x, 128, activation='relu', kern_reg=KR, dropout = 0.3, padding='same')\n",
        "    x = double_conv_module(x, 128, activation='relu', kern_reg=KR, dropout = 0.4, padding='same')\n",
        "   \n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu',kernel_regularizer=None)(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(classes)(x)\n",
        "    x = Activation(\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, x, name=\"rav_net\")\n",
        "    return model\n",
        "\n",
        "def fmp_unit(input, num_filters, dropout, padding=\"same\", frac_pool=True):\n",
        "    input = Conv2D(filters = num_filters, kernel_size = (3, 3), padding = padding, kernel_initializer='he_uniform')(input)\n",
        "    input = LeakyReLU()(input)\n",
        "    input = BatchNormalization(axis=-1)(input)\n",
        "    input = Conv2D(filters = num_filters, kernel_size = (3, 3), padding = padding, kernel_initializer='he_uniform')(input)\n",
        "    input = LeakyReLU()(input)\n",
        "    input = BatchNormalization(axis=-1)(input)\n",
        "    input = Lambda(frac_max_pool)(input) if frac_pool else input\n",
        "    input = Dropout(dropout)(input)\n",
        "\n",
        "    return input\n",
        "\n",
        "def fmp_model(width, height, depth, classes):\n",
        "    inputShape=(height, width, depth)\n",
        "    inputs = Input(shape=inputShape)\n",
        "\n",
        "    x = fmp_unit(inputs, 32, dropout = 0.3, padding='same', frac_pool=False)\n",
        "    x = fmp_unit(x, 64, dropout = 0.35, padding='same')\n",
        "    x = fmp_unit(x, 96, dropout = 0.35, padding='same')\n",
        "    x = fmp_unit(x, 128, dropout = 0.4, padding='same')\n",
        "    x = fmp_unit(x, 160, dropout = 0.45, padding='same')\n",
        "    x = fmp_unit(x, 192, dropout = 0.5, padding='same')\n",
        "\n",
        "    x = Conv2D(filters=192, kernel_size=(1, 1), padding='same', kernel_initializer='he_uniform')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(units=classes, kernel_initializer='he_uniform')(x)\n",
        "    x = Activation(\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, x, name=\"fmp_rav_net\")\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  if DATASET == 'CIFAR10':\n",
        "    data = Data10()\n",
        "    NUM_CLASSES = 10\n",
        "  else:\n",
        "    data = Data100()\n",
        "    NUM_CLASSES = 20\n",
        "      \n",
        "  x_train, y_train = data.x_train, data.y_train\n",
        "  x_test, y_test = data.x_test, data.y_test\n",
        "  x_val, y_val = data.x_val, data.y_val\n",
        "\n",
        "  lr_scheduler = LearningRateScheduler(poly_decay)\n",
        "  variable_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor = 0.2, patience = 2)\n",
        "\n",
        "  if MODEL == \"model3x\":\n",
        "    model3x = rav_model(width=32, height=32, depth=3, classes=NUM_CLASSES)\n",
        "    ac='relu'\n",
        "    adm=Adam(learning_rate=0.001,decay=0, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "    opt=adm\n",
        "    model3x.compile(loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy', TopKCategoricalAccuracy(k = 5)],optimizer=opt)\n",
        "    model3x.summary()\n",
        "  \n",
        "    history=model3x.fit(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE), \n",
        "                      batch_size=BATCH_SIZE, \n",
        "                      epochs=EPOCHS, \n",
        "                      callbacks=[variable_learning_rate, lr_scheduler], \n",
        "                      validation_data=(x_val, y_val), \n",
        "                      verbose=1, \n",
        "                      steps_per_epoch = len(x_train) // BATCH_SIZE)\n",
        "    score = model3x.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "  elif MODEL == \"fmp\":\n",
        "    model = fmp_model(width=32, height=32, depth=3, classes=10)\n",
        "    opt = RMSprop(decay=1e-6)\n",
        "    model.compile(loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'],optimizer=opt)\n",
        "    model.summary()\n",
        "    history=model.fit(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE), \n",
        "                      batch_size=BATCH_SIZE, \n",
        "                      epochs=EPOCHS, \n",
        "                      callbacks=[lr_scheduler], \n",
        "                      validation_data=(x_val, y_val), \n",
        "                      verbose=1, \n",
        "                      steps_per_epoch = len(x_train) // BATCH_SIZE)\n",
        "    score = model.evaluate(x_test, y_test, verbose=0)\n",
        "    \n",
        "\n",
        "  print('Test loss:', score[0])\n",
        "  print('Test accuracy:', score[1])\n",
        "\n",
        "  # 30 epochs 82%"
      ]
    }
  ]
}