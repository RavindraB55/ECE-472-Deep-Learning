{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nia9qnBerH12",
        "outputId": "f992c26a-3bfa-45cb-a15c-a8dcc84ae963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Data:\n",
            "Batch 1\n",
            "Batch 2\n",
            "Batch 3\n",
            "Batch 4\n",
            "Batch 5\n",
            "Model: \"rav_net\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 4, 4, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 4, 4, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 4, 4, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 4, 4, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 2, 2, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 2, 2, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               262656    \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 854,826\n",
            "Trainable params: 852,394\n",
            "Non-trainable params: 2,432\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "312/312 [==============================] - 35s 67ms/step - loss: 1.8938 - accuracy: 0.3562 - val_loss: 1.8340 - val_accuracy: 0.4146 - lr: 0.0050\n",
            "Epoch 2/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 1.4649 - accuracy: 0.4864 - val_loss: 1.7454 - val_accuracy: 0.4555 - lr: 0.0050\n",
            "Epoch 3/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 1.2222 - accuracy: 0.5714 - val_loss: 1.3573 - val_accuracy: 0.5847 - lr: 0.0049\n",
            "Epoch 4/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 1.0167 - accuracy: 0.6432 - val_loss: 1.0941 - val_accuracy: 0.6520 - lr: 0.0049\n",
            "Epoch 5/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.9078 - accuracy: 0.6858 - val_loss: 0.8463 - val_accuracy: 0.7113 - lr: 0.0049\n",
            "Epoch 6/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.8308 - accuracy: 0.7166 - val_loss: 0.9372 - val_accuracy: 0.6922 - lr: 0.0049\n",
            "Epoch 7/200\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.7875 - accuracy: 0.7299 - val_loss: 0.7228 - val_accuracy: 0.7577 - lr: 0.0049\n",
            "Epoch 8/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.7387 - accuracy: 0.7454 - val_loss: 0.7905 - val_accuracy: 0.7324 - lr: 0.0048\n",
            "Epoch 9/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.7094 - accuracy: 0.7583 - val_loss: 0.6589 - val_accuracy: 0.7825 - lr: 0.0048\n",
            "Epoch 10/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.6691 - accuracy: 0.7714 - val_loss: 0.7087 - val_accuracy: 0.7729 - lr: 0.0048\n",
            "Epoch 11/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.6515 - accuracy: 0.7756 - val_loss: 0.5979 - val_accuracy: 0.7963 - lr: 0.0047\n",
            "Epoch 12/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.6272 - accuracy: 0.7865 - val_loss: 0.6182 - val_accuracy: 0.7934 - lr: 0.0047\n",
            "Epoch 13/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.6155 - accuracy: 0.7893 - val_loss: 0.6314 - val_accuracy: 0.7906 - lr: 9.4000e-04\n",
            "Epoch 14/200\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.5935 - accuracy: 0.7981 - val_loss: 0.6520 - val_accuracy: 0.7954 - lr: 0.0047\n",
            "Epoch 15/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.5751 - accuracy: 0.8035 - val_loss: 0.5686 - val_accuracy: 0.8118 - lr: 0.0047\n",
            "Epoch 16/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.5605 - accuracy: 0.8086 - val_loss: 0.6175 - val_accuracy: 0.8028 - lr: 0.0046\n",
            "Epoch 17/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.5435 - accuracy: 0.8139 - val_loss: 0.6163 - val_accuracy: 0.8003 - lr: 9.2000e-04\n",
            "Epoch 18/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.5327 - accuracy: 0.8173 - val_loss: 0.5585 - val_accuracy: 0.8182 - lr: 0.0046\n",
            "Epoch 19/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.5238 - accuracy: 0.8201 - val_loss: 0.5485 - val_accuracy: 0.8173 - lr: 0.0046\n",
            "Epoch 20/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.5058 - accuracy: 0.8262 - val_loss: 0.4288 - val_accuracy: 0.8534 - lr: 0.0045\n",
            "Epoch 21/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4945 - accuracy: 0.8334 - val_loss: 0.4424 - val_accuracy: 0.8475 - lr: 0.0045\n",
            "Epoch 22/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4876 - accuracy: 0.8343 - val_loss: 0.4942 - val_accuracy: 0.8391 - lr: 8.9500e-04\n",
            "Epoch 23/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4811 - accuracy: 0.8361 - val_loss: 0.5253 - val_accuracy: 0.8332 - lr: 0.0044\n",
            "Epoch 24/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4692 - accuracy: 0.8405 - val_loss: 0.5057 - val_accuracy: 0.8324 - lr: 8.8500e-04\n",
            "Epoch 25/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4646 - accuracy: 0.8412 - val_loss: 0.4690 - val_accuracy: 0.8470 - lr: 0.0044\n",
            "Epoch 26/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4509 - accuracy: 0.8451 - val_loss: 0.5268 - val_accuracy: 0.8258 - lr: 8.7500e-04\n",
            "Epoch 27/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.4461 - accuracy: 0.8462 - val_loss: 0.4930 - val_accuracy: 0.8375 - lr: 0.0044\n",
            "Epoch 28/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4337 - accuracy: 0.8518 - val_loss: 0.4281 - val_accuracy: 0.8572 - lr: 0.0043\n",
            "Epoch 29/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4287 - accuracy: 0.8554 - val_loss: 0.4160 - val_accuracy: 0.8627 - lr: 0.0043\n",
            "Epoch 30/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4208 - accuracy: 0.8557 - val_loss: 0.4173 - val_accuracy: 0.8618 - lr: 0.0043\n",
            "Epoch 31/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4180 - accuracy: 0.8580 - val_loss: 0.4539 - val_accuracy: 0.8548 - lr: 8.5000e-04\n",
            "Epoch 32/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.4103 - accuracy: 0.8593 - val_loss: 0.4961 - val_accuracy: 0.8453 - lr: 0.0042\n",
            "Epoch 33/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3965 - accuracy: 0.8648 - val_loss: 0.4200 - val_accuracy: 0.8599 - lr: 8.4000e-04\n",
            "Epoch 34/200\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.3976 - accuracy: 0.8635 - val_loss: 0.4425 - val_accuracy: 0.8598 - lr: 0.0042\n",
            "Epoch 35/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3933 - accuracy: 0.8662 - val_loss: 0.4573 - val_accuracy: 0.8571 - lr: 8.3000e-04\n",
            "Epoch 36/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3839 - accuracy: 0.8698 - val_loss: 0.4219 - val_accuracy: 0.8630 - lr: 0.0041\n",
            "Epoch 37/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3769 - accuracy: 0.8711 - val_loss: 0.3885 - val_accuracy: 0.8697 - lr: 0.0041\n",
            "Epoch 38/200\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.3725 - accuracy: 0.8735 - val_loss: 0.3874 - val_accuracy: 0.8764 - lr: 0.0041\n",
            "Epoch 39/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3753 - accuracy: 0.8721 - val_loss: 0.3908 - val_accuracy: 0.8704 - lr: 0.0041\n",
            "Epoch 40/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3656 - accuracy: 0.8751 - val_loss: 0.3802 - val_accuracy: 0.8749 - lr: 0.0040\n",
            "Epoch 41/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3613 - accuracy: 0.8758 - val_loss: 0.3852 - val_accuracy: 0.8719 - lr: 0.0040\n",
            "Epoch 42/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3554 - accuracy: 0.8771 - val_loss: 0.3990 - val_accuracy: 0.8677 - lr: 7.9500e-04\n",
            "Epoch 43/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3516 - accuracy: 0.8788 - val_loss: 0.3552 - val_accuracy: 0.8815 - lr: 0.0039\n",
            "Epoch 44/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3476 - accuracy: 0.8791 - val_loss: 0.4061 - val_accuracy: 0.8702 - lr: 0.0039\n",
            "Epoch 45/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3496 - accuracy: 0.8795 - val_loss: 0.4009 - val_accuracy: 0.8703 - lr: 7.8000e-04\n",
            "Epoch 46/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3391 - accuracy: 0.8833 - val_loss: 0.3568 - val_accuracy: 0.8833 - lr: 0.0039\n",
            "Epoch 47/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3383 - accuracy: 0.8848 - val_loss: 0.3546 - val_accuracy: 0.8822 - lr: 0.0038\n",
            "Epoch 48/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3319 - accuracy: 0.8862 - val_loss: 0.3982 - val_accuracy: 0.8727 - lr: 0.0038\n",
            "Epoch 49/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3301 - accuracy: 0.8867 - val_loss: 0.3747 - val_accuracy: 0.8777 - lr: 7.6000e-04\n",
            "Epoch 50/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3263 - accuracy: 0.8891 - val_loss: 0.3679 - val_accuracy: 0.8825 - lr: 0.0038\n",
            "Epoch 51/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3256 - accuracy: 0.8884 - val_loss: 0.3621 - val_accuracy: 0.8798 - lr: 7.5000e-04\n",
            "Epoch 52/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3194 - accuracy: 0.8892 - val_loss: 0.3658 - val_accuracy: 0.8807 - lr: 0.0037\n",
            "Epoch 53/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.3208 - accuracy: 0.8899 - val_loss: 0.3881 - val_accuracy: 0.8781 - lr: 7.4000e-04\n",
            "Epoch 54/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.3106 - accuracy: 0.8943 - val_loss: 0.3356 - val_accuracy: 0.8908 - lr: 0.0037\n",
            "Epoch 55/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3094 - accuracy: 0.8936 - val_loss: 0.3625 - val_accuracy: 0.8841 - lr: 0.0037\n",
            "Epoch 56/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3028 - accuracy: 0.8966 - val_loss: 0.3493 - val_accuracy: 0.8843 - lr: 7.2500e-04\n",
            "Epoch 57/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3067 - accuracy: 0.8941 - val_loss: 0.3751 - val_accuracy: 0.8790 - lr: 0.0036\n",
            "Epoch 58/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3050 - accuracy: 0.8953 - val_loss: 0.3653 - val_accuracy: 0.8835 - lr: 7.1500e-04\n",
            "Epoch 59/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2989 - accuracy: 0.8972 - val_loss: 0.3463 - val_accuracy: 0.8854 - lr: 0.0036\n",
            "Epoch 60/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.3012 - accuracy: 0.8950 - val_loss: 0.4127 - val_accuracy: 0.8721 - lr: 7.0500e-04\n",
            "Epoch 61/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2944 - accuracy: 0.8995 - val_loss: 0.3842 - val_accuracy: 0.8763 - lr: 0.0035\n",
            "Epoch 62/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2927 - accuracy: 0.8983 - val_loss: 0.3563 - val_accuracy: 0.8876 - lr: 6.9500e-04\n",
            "Epoch 63/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2871 - accuracy: 0.9022 - val_loss: 0.3435 - val_accuracy: 0.8893 - lr: 0.0034\n",
            "Epoch 64/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2866 - accuracy: 0.9023 - val_loss: 0.3589 - val_accuracy: 0.8910 - lr: 6.8500e-04\n",
            "Epoch 65/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2802 - accuracy: 0.9043 - val_loss: 0.3494 - val_accuracy: 0.8878 - lr: 0.0034\n",
            "Epoch 66/200\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.2800 - accuracy: 0.9041 - val_loss: 0.3695 - val_accuracy: 0.8838 - lr: 6.7500e-04\n",
            "Epoch 67/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2762 - accuracy: 0.9055 - val_loss: 0.3863 - val_accuracy: 0.8774 - lr: 0.0033\n",
            "Epoch 68/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2748 - accuracy: 0.9049 - val_loss: 0.3643 - val_accuracy: 0.8847 - lr: 6.6500e-04\n",
            "Epoch 69/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2744 - accuracy: 0.9059 - val_loss: 0.3258 - val_accuracy: 0.8946 - lr: 0.0033\n",
            "Epoch 70/200\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.2726 - accuracy: 0.9066 - val_loss: 0.3638 - val_accuracy: 0.8864 - lr: 0.0033\n",
            "Epoch 71/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2688 - accuracy: 0.9063 - val_loss: 0.3313 - val_accuracy: 0.8940 - lr: 6.5000e-04\n",
            "Epoch 72/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2685 - accuracy: 0.9061 - val_loss: 0.3757 - val_accuracy: 0.8851 - lr: 0.0032\n",
            "Epoch 73/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2643 - accuracy: 0.9085 - val_loss: 0.3601 - val_accuracy: 0.8872 - lr: 6.4000e-04\n",
            "Epoch 74/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2579 - accuracy: 0.9110 - val_loss: 0.3344 - val_accuracy: 0.8941 - lr: 0.0032\n",
            "Epoch 75/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2564 - accuracy: 0.9112 - val_loss: 0.3361 - val_accuracy: 0.8960 - lr: 6.3000e-04\n",
            "Epoch 76/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2634 - accuracy: 0.9099 - val_loss: 0.3411 - val_accuracy: 0.8917 - lr: 0.0031\n",
            "Epoch 77/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2544 - accuracy: 0.9112 - val_loss: 0.3445 - val_accuracy: 0.8954 - lr: 6.2000e-04\n",
            "Epoch 78/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2536 - accuracy: 0.9126 - val_loss: 0.3619 - val_accuracy: 0.8870 - lr: 0.0031\n",
            "Epoch 79/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2413 - accuracy: 0.9157 - val_loss: 0.3416 - val_accuracy: 0.8928 - lr: 6.1000e-04\n",
            "Epoch 80/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2459 - accuracy: 0.9154 - val_loss: 0.3449 - val_accuracy: 0.8956 - lr: 0.0030\n",
            "Epoch 81/200\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.2442 - accuracy: 0.9155 - val_loss: 0.3710 - val_accuracy: 0.8870 - lr: 6.0000e-04\n",
            "Epoch 82/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2446 - accuracy: 0.9154 - val_loss: 0.3733 - val_accuracy: 0.8865 - lr: 0.0030\n",
            "Epoch 83/200\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.2427 - accuracy: 0.9152 - val_loss: 0.3426 - val_accuracy: 0.8923 - lr: 5.9000e-04\n",
            "Epoch 84/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2421 - accuracy: 0.9140 - val_loss: 0.3461 - val_accuracy: 0.8912 - lr: 0.0029\n",
            "Epoch 85/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2396 - accuracy: 0.9162 - val_loss: 0.3460 - val_accuracy: 0.8903 - lr: 5.8000e-04\n",
            "Epoch 86/200\n",
            "312/312 [==============================] - 20s 63ms/step - loss: 0.2434 - accuracy: 0.9164 - val_loss: 0.3473 - val_accuracy: 0.8920 - lr: 0.0029\n",
            "Epoch 87/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2376 - accuracy: 0.9186 - val_loss: 0.3346 - val_accuracy: 0.8939 - lr: 5.7000e-04\n",
            "Epoch 88/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2327 - accuracy: 0.9192 - val_loss: 0.3415 - val_accuracy: 0.8938 - lr: 0.0028\n",
            "Epoch 89/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2257 - accuracy: 0.9218 - val_loss: 0.3477 - val_accuracy: 0.8907 - lr: 5.6000e-04\n",
            "Epoch 90/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2316 - accuracy: 0.9189 - val_loss: 0.3323 - val_accuracy: 0.8948 - lr: 0.0028\n",
            "Epoch 91/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2305 - accuracy: 0.9206 - val_loss: 0.3244 - val_accuracy: 0.8980 - lr: 0.0027\n",
            "Epoch 92/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2274 - accuracy: 0.9207 - val_loss: 0.3492 - val_accuracy: 0.8937 - lr: 0.0027\n",
            "Epoch 93/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2216 - accuracy: 0.9230 - val_loss: 0.3531 - val_accuracy: 0.8902 - lr: 5.4000e-04\n",
            "Epoch 94/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2269 - accuracy: 0.9206 - val_loss: 0.3217 - val_accuracy: 0.8976 - lr: 0.0027\n",
            "Epoch 95/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2248 - accuracy: 0.9235 - val_loss: 0.3538 - val_accuracy: 0.8915 - lr: 0.0026\n",
            "Epoch 96/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2227 - accuracy: 0.9221 - val_loss: 0.3355 - val_accuracy: 0.8931 - lr: 5.2500e-04\n",
            "Epoch 97/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2205 - accuracy: 0.9230 - val_loss: 0.3348 - val_accuracy: 0.8970 - lr: 0.0026\n",
            "Epoch 98/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2191 - accuracy: 0.9235 - val_loss: 0.3369 - val_accuracy: 0.8986 - lr: 5.1500e-04\n",
            "Epoch 99/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2167 - accuracy: 0.9246 - val_loss: 0.3487 - val_accuracy: 0.8922 - lr: 0.0026\n",
            "Epoch 100/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2090 - accuracy: 0.9276 - val_loss: 0.3311 - val_accuracy: 0.9001 - lr: 5.0500e-04\n",
            "Epoch 101/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2172 - accuracy: 0.9251 - val_loss: 0.3480 - val_accuracy: 0.8953 - lr: 0.0025\n",
            "Epoch 102/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2117 - accuracy: 0.9270 - val_loss: 0.3560 - val_accuracy: 0.8915 - lr: 4.9500e-04\n",
            "Epoch 103/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2104 - accuracy: 0.9271 - val_loss: 0.3538 - val_accuracy: 0.8920 - lr: 0.0025\n",
            "Epoch 104/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2102 - accuracy: 0.9268 - val_loss: 0.3263 - val_accuracy: 0.8979 - lr: 4.8500e-04\n",
            "Epoch 105/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2064 - accuracy: 0.9269 - val_loss: 0.3309 - val_accuracy: 0.8979 - lr: 0.0024\n",
            "Epoch 106/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2091 - accuracy: 0.9284 - val_loss: 0.3150 - val_accuracy: 0.9019 - lr: 0.0024\n",
            "Epoch 107/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2002 - accuracy: 0.9316 - val_loss: 0.3235 - val_accuracy: 0.9007 - lr: 0.0023\n",
            "Epoch 108/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.2060 - accuracy: 0.9280 - val_loss: 0.3204 - val_accuracy: 0.9028 - lr: 4.6500e-04\n",
            "Epoch 109/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1982 - accuracy: 0.9318 - val_loss: 0.3109 - val_accuracy: 0.9037 - lr: 0.0023\n",
            "Epoch 110/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2023 - accuracy: 0.9302 - val_loss: 0.3379 - val_accuracy: 0.8971 - lr: 0.0023\n",
            "Epoch 111/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1981 - accuracy: 0.9317 - val_loss: 0.3478 - val_accuracy: 0.8967 - lr: 4.5000e-04\n",
            "Epoch 112/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.2025 - accuracy: 0.9284 - val_loss: 0.3599 - val_accuracy: 0.8935 - lr: 0.0022\n",
            "Epoch 113/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1981 - accuracy: 0.9319 - val_loss: 0.3227 - val_accuracy: 0.9017 - lr: 4.4000e-04\n",
            "Epoch 114/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1919 - accuracy: 0.9343 - val_loss: 0.3575 - val_accuracy: 0.8939 - lr: 0.0022\n",
            "Epoch 115/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1944 - accuracy: 0.9316 - val_loss: 0.3187 - val_accuracy: 0.9055 - lr: 4.3000e-04\n",
            "Epoch 116/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1930 - accuracy: 0.9319 - val_loss: 0.3319 - val_accuracy: 0.8987 - lr: 0.0021\n",
            "Epoch 117/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1899 - accuracy: 0.9343 - val_loss: 0.3475 - val_accuracy: 0.8967 - lr: 4.2000e-04\n",
            "Epoch 118/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1913 - accuracy: 0.9338 - val_loss: 0.3258 - val_accuracy: 0.9008 - lr: 0.0021\n",
            "Epoch 119/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1874 - accuracy: 0.9332 - val_loss: 0.3407 - val_accuracy: 0.9001 - lr: 4.1000e-04\n",
            "Epoch 120/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1872 - accuracy: 0.9355 - val_loss: 0.3153 - val_accuracy: 0.9026 - lr: 0.0020\n",
            "Epoch 121/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1860 - accuracy: 0.9352 - val_loss: 0.3558 - val_accuracy: 0.8943 - lr: 4.0000e-04\n",
            "Epoch 122/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1870 - accuracy: 0.9340 - val_loss: 0.3275 - val_accuracy: 0.9012 - lr: 0.0020\n",
            "Epoch 123/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1806 - accuracy: 0.9377 - val_loss: 0.3265 - val_accuracy: 0.9029 - lr: 3.9000e-04\n",
            "Epoch 124/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1836 - accuracy: 0.9356 - val_loss: 0.3332 - val_accuracy: 0.9004 - lr: 0.0019\n",
            "Epoch 125/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1824 - accuracy: 0.9366 - val_loss: 0.3237 - val_accuracy: 0.9023 - lr: 3.8000e-04\n",
            "Epoch 126/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1814 - accuracy: 0.9366 - val_loss: 0.3171 - val_accuracy: 0.9068 - lr: 0.0019\n",
            "Epoch 127/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1770 - accuracy: 0.9387 - val_loss: 0.3275 - val_accuracy: 0.9009 - lr: 3.7000e-04\n",
            "Epoch 128/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1755 - accuracy: 0.9373 - val_loss: 0.3419 - val_accuracy: 0.8974 - lr: 0.0018\n",
            "Epoch 129/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1778 - accuracy: 0.9379 - val_loss: 0.3232 - val_accuracy: 0.9025 - lr: 3.6000e-04\n",
            "Epoch 130/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1714 - accuracy: 0.9407 - val_loss: 0.3175 - val_accuracy: 0.9057 - lr: 0.0018\n",
            "Epoch 131/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1789 - accuracy: 0.9377 - val_loss: 0.3204 - val_accuracy: 0.9013 - lr: 3.5000e-04\n",
            "Epoch 132/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1727 - accuracy: 0.9387 - val_loss: 0.3303 - val_accuracy: 0.9020 - lr: 0.0017\n",
            "Epoch 133/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1711 - accuracy: 0.9414 - val_loss: 0.3059 - val_accuracy: 0.9075 - lr: 0.0017\n",
            "Epoch 134/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1704 - accuracy: 0.9410 - val_loss: 0.3379 - val_accuracy: 0.9014 - lr: 0.0017\n",
            "Epoch 135/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1676 - accuracy: 0.9416 - val_loss: 0.3416 - val_accuracy: 0.8995 - lr: 3.3000e-04\n",
            "Epoch 136/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1682 - accuracy: 0.9427 - val_loss: 0.3247 - val_accuracy: 0.9032 - lr: 0.0016\n",
            "Epoch 137/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1685 - accuracy: 0.9415 - val_loss: 0.3248 - val_accuracy: 0.9016 - lr: 3.2000e-04\n",
            "Epoch 138/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1624 - accuracy: 0.9424 - val_loss: 0.3305 - val_accuracy: 0.9014 - lr: 0.0016\n",
            "Epoch 139/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1624 - accuracy: 0.9428 - val_loss: 0.3296 - val_accuracy: 0.9022 - lr: 3.1000e-04\n",
            "Epoch 140/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1615 - accuracy: 0.9427 - val_loss: 0.3317 - val_accuracy: 0.9047 - lr: 0.0015\n",
            "Epoch 141/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1599 - accuracy: 0.9443 - val_loss: 0.3199 - val_accuracy: 0.9082 - lr: 3.0000e-04\n",
            "Epoch 142/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1614 - accuracy: 0.9448 - val_loss: 0.3202 - val_accuracy: 0.9073 - lr: 0.0015\n",
            "Epoch 143/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1602 - accuracy: 0.9429 - val_loss: 0.3431 - val_accuracy: 0.9007 - lr: 2.9000e-04\n",
            "Epoch 144/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1576 - accuracy: 0.9458 - val_loss: 0.3302 - val_accuracy: 0.9060 - lr: 0.0014\n",
            "Epoch 145/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1605 - accuracy: 0.9436 - val_loss: 0.3346 - val_accuracy: 0.9040 - lr: 2.8000e-04\n",
            "Epoch 146/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1573 - accuracy: 0.9440 - val_loss: 0.3190 - val_accuracy: 0.9058 - lr: 0.0014\n",
            "Epoch 147/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1595 - accuracy: 0.9443 - val_loss: 0.3261 - val_accuracy: 0.9048 - lr: 2.7000e-04\n",
            "Epoch 148/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1554 - accuracy: 0.9458 - val_loss: 0.3113 - val_accuracy: 0.9093 - lr: 0.0013\n",
            "Epoch 149/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1571 - accuracy: 0.9457 - val_loss: 0.3099 - val_accuracy: 0.9077 - lr: 2.6000e-04\n",
            "Epoch 150/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1557 - accuracy: 0.9452 - val_loss: 0.3264 - val_accuracy: 0.9049 - lr: 0.0013\n",
            "Epoch 151/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1499 - accuracy: 0.9480 - val_loss: 0.3206 - val_accuracy: 0.9086 - lr: 2.5000e-04\n",
            "Epoch 152/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1528 - accuracy: 0.9468 - val_loss: 0.3202 - val_accuracy: 0.9064 - lr: 0.0012\n",
            "Epoch 153/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1524 - accuracy: 0.9467 - val_loss: 0.3162 - val_accuracy: 0.9077 - lr: 2.4000e-04\n",
            "Epoch 154/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1527 - accuracy: 0.9473 - val_loss: 0.3209 - val_accuracy: 0.9072 - lr: 0.0012\n",
            "Epoch 155/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1504 - accuracy: 0.9461 - val_loss: 0.3206 - val_accuracy: 0.9068 - lr: 2.3000e-04\n",
            "Epoch 156/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1466 - accuracy: 0.9490 - val_loss: 0.3140 - val_accuracy: 0.9095 - lr: 0.0011\n",
            "Epoch 157/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1462 - accuracy: 0.9489 - val_loss: 0.3234 - val_accuracy: 0.9051 - lr: 2.2000e-04\n",
            "Epoch 158/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1440 - accuracy: 0.9485 - val_loss: 0.3140 - val_accuracy: 0.9103 - lr: 0.0011\n",
            "Epoch 159/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1474 - accuracy: 0.9487 - val_loss: 0.3247 - val_accuracy: 0.9066 - lr: 2.1000e-04\n",
            "Epoch 160/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1444 - accuracy: 0.9489 - val_loss: 0.3266 - val_accuracy: 0.9070 - lr: 0.0010\n",
            "Epoch 161/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1410 - accuracy: 0.9504 - val_loss: 0.3375 - val_accuracy: 0.9056 - lr: 2.0000e-04\n",
            "Epoch 162/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1449 - accuracy: 0.9490 - val_loss: 0.3266 - val_accuracy: 0.9062 - lr: 9.7500e-04\n",
            "Epoch 163/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1430 - accuracy: 0.9503 - val_loss: 0.3251 - val_accuracy: 0.9064 - lr: 1.9000e-04\n",
            "Epoch 164/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1411 - accuracy: 0.9509 - val_loss: 0.3354 - val_accuracy: 0.9052 - lr: 9.2500e-04\n",
            "Epoch 165/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1408 - accuracy: 0.9502 - val_loss: 0.3310 - val_accuracy: 0.9044 - lr: 1.8000e-04\n",
            "Epoch 166/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1366 - accuracy: 0.9521 - val_loss: 0.3250 - val_accuracy: 0.9064 - lr: 8.7500e-04\n",
            "Epoch 167/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1383 - accuracy: 0.9516 - val_loss: 0.3348 - val_accuracy: 0.9047 - lr: 1.7000e-04\n",
            "Epoch 168/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1358 - accuracy: 0.9515 - val_loss: 0.3245 - val_accuracy: 0.9065 - lr: 8.2500e-04\n",
            "Epoch 169/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1361 - accuracy: 0.9515 - val_loss: 0.3202 - val_accuracy: 0.9093 - lr: 1.6000e-04\n",
            "Epoch 170/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1333 - accuracy: 0.9546 - val_loss: 0.3198 - val_accuracy: 0.9084 - lr: 7.7500e-04\n",
            "Epoch 171/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1340 - accuracy: 0.9526 - val_loss: 0.3294 - val_accuracy: 0.9076 - lr: 1.5000e-04\n",
            "Epoch 172/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1346 - accuracy: 0.9528 - val_loss: 0.3227 - val_accuracy: 0.9063 - lr: 7.2500e-04\n",
            "Epoch 173/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1344 - accuracy: 0.9535 - val_loss: 0.3292 - val_accuracy: 0.9079 - lr: 1.4000e-04\n",
            "Epoch 174/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1363 - accuracy: 0.9532 - val_loss: 0.3149 - val_accuracy: 0.9092 - lr: 6.7500e-04\n",
            "Epoch 175/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1300 - accuracy: 0.9554 - val_loss: 0.3221 - val_accuracy: 0.9112 - lr: 1.3000e-04\n",
            "Epoch 176/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1312 - accuracy: 0.9551 - val_loss: 0.3118 - val_accuracy: 0.9100 - lr: 6.2500e-04\n",
            "Epoch 177/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1266 - accuracy: 0.9559 - val_loss: 0.3185 - val_accuracy: 0.9110 - lr: 1.2000e-04\n",
            "Epoch 178/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1275 - accuracy: 0.9550 - val_loss: 0.3331 - val_accuracy: 0.9063 - lr: 5.7500e-04\n",
            "Epoch 179/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1305 - accuracy: 0.9536 - val_loss: 0.3218 - val_accuracy: 0.9096 - lr: 1.1000e-04\n",
            "Epoch 180/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1250 - accuracy: 0.9554 - val_loss: 0.3132 - val_accuracy: 0.9124 - lr: 5.2500e-04\n",
            "Epoch 181/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1258 - accuracy: 0.9560 - val_loss: 0.3190 - val_accuracy: 0.9106 - lr: 1.0000e-04\n",
            "Epoch 182/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1288 - accuracy: 0.9556 - val_loss: 0.3214 - val_accuracy: 0.9102 - lr: 4.7500e-04\n",
            "Epoch 183/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1260 - accuracy: 0.9555 - val_loss: 0.3272 - val_accuracy: 0.9072 - lr: 9.0000e-05\n",
            "Epoch 184/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1267 - accuracy: 0.9553 - val_loss: 0.3249 - val_accuracy: 0.9093 - lr: 4.2500e-04\n",
            "Epoch 185/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1236 - accuracy: 0.9561 - val_loss: 0.3282 - val_accuracy: 0.9074 - lr: 8.0000e-05\n",
            "Epoch 186/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1280 - accuracy: 0.9552 - val_loss: 0.3233 - val_accuracy: 0.9093 - lr: 3.7500e-04\n",
            "Epoch 187/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1249 - accuracy: 0.9563 - val_loss: 0.3209 - val_accuracy: 0.9103 - lr: 7.0000e-05\n",
            "Epoch 188/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1263 - accuracy: 0.9554 - val_loss: 0.3135 - val_accuracy: 0.9126 - lr: 3.2500e-04\n",
            "Epoch 189/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1261 - accuracy: 0.9559 - val_loss: 0.3166 - val_accuracy: 0.9119 - lr: 6.0000e-05\n",
            "Epoch 190/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1220 - accuracy: 0.9576 - val_loss: 0.3223 - val_accuracy: 0.9089 - lr: 2.7500e-04\n",
            "Epoch 191/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1241 - accuracy: 0.9567 - val_loss: 0.3203 - val_accuracy: 0.9096 - lr: 5.0000e-05\n",
            "Epoch 192/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1201 - accuracy: 0.9581 - val_loss: 0.3213 - val_accuracy: 0.9105 - lr: 2.2500e-04\n",
            "Epoch 193/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1203 - accuracy: 0.9572 - val_loss: 0.3184 - val_accuracy: 0.9124 - lr: 4.0000e-05\n",
            "Epoch 194/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1176 - accuracy: 0.9584 - val_loss: 0.3199 - val_accuracy: 0.9126 - lr: 1.7500e-04\n",
            "Epoch 195/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1194 - accuracy: 0.9573 - val_loss: 0.3206 - val_accuracy: 0.9120 - lr: 3.0000e-05\n",
            "Epoch 196/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1189 - accuracy: 0.9571 - val_loss: 0.3199 - val_accuracy: 0.9119 - lr: 1.2500e-04\n",
            "Epoch 197/200\n",
            "312/312 [==============================] - 20s 64ms/step - loss: 0.1182 - accuracy: 0.9585 - val_loss: 0.3207 - val_accuracy: 0.9120 - lr: 2.0000e-05\n",
            "Epoch 198/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1190 - accuracy: 0.9577 - val_loss: 0.3200 - val_accuracy: 0.9111 - lr: 7.5000e-05\n",
            "Epoch 199/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1166 - accuracy: 0.9588 - val_loss: 0.3221 - val_accuracy: 0.9109 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "312/312 [==============================] - 20s 65ms/step - loss: 0.1147 - accuracy: 0.9600 - val_loss: 0.3202 - val_accuracy: 0.9114 - lr: 2.5000e-05\n",
            "Test loss: 0.3363804817199707\n",
            "Test accuracy: 0.907800018787384\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, MaxPool2D, Flatten, Add, Dense, Activation, BatchNormalization, Lambda, ReLU, PReLU, LeakyReLU, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
        "from tensorflow.keras.layers import Flatten, Input, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\"\"\"\n",
        "The CIFAR10 dataset was downloaded through the official website, with each training batch being unpickled and then appended to each other\n",
        "to create one large training set. The images were preprocessed to convert the initial row vector to shape (32, 32, 3) through reshaping and transposing.\n",
        "The class output data was one hot encoded. My preliminary attempt used my model for the MNIST dataset, with an alteration for the input size. This\n",
        "resulted in a test accuracy of 67% after 10 epochs. My next attempt was a VGG with fractional max pooling, based on a paper by Benjamin Graham. While this definitely \n",
        "outperformed the previous model, the computational time was far too high. I tried reducing training time by using only one of the 5 training batches and doubling the \n",
        "batch size to 512, but the tradeoff with accuracy was way too high. \n",
        "\n",
        "The next model I tried was another VGG type convolutional network, which was shallower and converged much faster. This got me to 80% test accuracy.   \n",
        "\"\"\"\n",
        "# Fractional max pooling\n",
        "# - https://arxiv.org/abs/1412.6071\n",
        "# - https://github.com/laplacetw/vgg-like-cifar10/blob/master/fmp_cifar10.py\n",
        "# https://www.binarystudy.com/2021/09/how-to-load-preprocess-visualize-CIFAR-10-and-CIFAR-100.html#routine\n",
        "\n",
        "BTEST = '../data/test_batch'\n",
        "meta_file = '../CIFAR10-data/batches.meta'\n",
        "\n",
        "NUM_TRAINING_BATCHES = 5\n",
        "BATCH_SIZE = 128 #128\n",
        "LAMBDA = 1e-5\n",
        "EPOCHS = 200\n",
        "IMG_SIDE_LEN = 32\n",
        "LR = 5e-3\n",
        "DATASET = \"CIFAR10\"\n",
        "MODEL = \"model3x\"\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        u = pickle._Unpickler( fo )\n",
        "        u.encoding = 'latin1'\n",
        "        dict = u.load()\n",
        "    return dict\n",
        "\n",
        "def load_training_data():\n",
        "    # The whole data_batch_1 has 10,000 images. And each image is a 1-D array having 3,072 entries. \n",
        "    # First 1024 entries for Red, the next 1024 entries for Green and last 1024 entries for Blue channels. \n",
        "    print(\"Loading Data:\")\n",
        "    features, classes = np.empty((0,32,32,3)), np.empty((0,10))\n",
        "    for i in range(NUM_TRAINING_BATCHES):\n",
        "        print(f\"Batch {i+1}\")\n",
        "        batch_path = f'../data/data_batch_{i+1}'\n",
        "        x, y = reshape_features(batch_path)\n",
        "        features = np.append(features, x, axis=0)\n",
        "        classes = np.append(classes, y, axis=0)\n",
        "    return features, classes\n",
        "\n",
        "def reshape_features(feat_path, CIFAR100=False):\n",
        "    labels = 'coarse_labels' if CIFAR100 else 'labels'\n",
        "    unpickled_data = unpickle(feat_path)\n",
        "    return (unpickled_data['data'].reshape(len(unpickled_data['data']),3,32,32).transpose(0,2,3,1) / 255,\n",
        "            tf.keras.utils.to_categorical(unpickled_data[labels]))\n",
        "\n",
        "def frac_max_pool(x):\n",
        "    return tf.nn.fractional_max_pool(x, [1.0, 1.41, 1.41, 1.0], pseudo_random=True, overlapping=True)[0]\n",
        "\n",
        "def poly_decay(epoch):\n",
        "  maxEpochs = EPOCHS\n",
        "  baseLR = LR\n",
        "  power = 1.0\n",
        "  alpha = baseLR * (1 - (epoch   / float(maxEpochs))) ** power\n",
        "  return alpha\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    horizontal_flip=True,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        "    #zoom_range=0.3\n",
        ")\n",
        "\n",
        "aug = ImageDataGenerator(width_shift_range=0.1,height_shift_range=0.1, horizontal_flip=True,fill_mode=\"nearest\")\n",
        "\n",
        "stopping = tf.keras.callbacks.EarlyStopping(\n",
        "          monitor=\"val_accuracy\",\n",
        "          min_delta=0,\n",
        "          patience=25,\n",
        "          verbose=1,\n",
        "          mode=\"max\",\n",
        "          baseline=None,\n",
        "          restore_best_weights=True)\n",
        "\n",
        "def normalize_x_data(x_train, x_test):\n",
        "    eps = 1e-7\n",
        "    mean = np.mean(x_train,axis = (0, 1, 2, 3))\n",
        "    std = np.std(x_train,axis = (0, 1, 2, 3))\n",
        "    x_train = (x_train - mean)/(std + eps)\n",
        "    x_test = (x_test - mean)/(std + eps)\n",
        "    return x_train, x_test\n",
        "\n",
        "class Data10(object):\n",
        "    def __init__(self):\n",
        "        self.x_train, self.y_train = load_training_data()\n",
        "        self.x_test, self.y_test = reshape_features(BTEST)\n",
        "        self.x_train, self.x_test = normalize_x_data(self.x_train, self.x_test)\n",
        "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train, self.y_train, test_size=0.2, random_state=31415)\n",
        "\n",
        "class Data100(object):\n",
        "    def __init__(self):\n",
        "      self.x_train, self.y_train = reshape_features('../data/train', CIFAR100=True)\n",
        "      self.x_test, self.y_test = reshape_features('../data/test', CIFAR100=True)\n",
        "      self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train, self.y_train, test_size=0.2, random_state=31415)\n",
        "\n",
        "def double_conv_module(input, num_filters, activation, kern_reg, dropout, padding=\"same\"):\n",
        "    input = Conv2D(filters = num_filters, kernel_size = (3, 3), activation = activation, padding = padding, kernel_regularizer = kern_reg)(input)\n",
        "    input = BatchNormalization(axis=-1)(input)\n",
        "    input = Conv2D(filters = num_filters, kernel_size = (3, 3), activation = activation, padding = padding, kernel_regularizer = kern_reg)(input)\n",
        "    input = BatchNormalization(axis=-1)(input)\n",
        "    input = MaxPooling2D(pool_size = (2, 2))(input)\n",
        "    input = Dropout(dropout)(input)\n",
        "\n",
        "    return input\n",
        "\n",
        "def rav_model(width, height, depth, classes):\n",
        "    inputShape=(height, width, depth)\n",
        "    weight_decay = 0.001\n",
        "\n",
        "    # (Step 1) Define the model input\n",
        "    inputs = Input(shape=inputShape)\n",
        "    KR = None #l2(weight_decay)\n",
        "    x = double_conv_module(inputs, 32, activation='relu', kern_reg=KR, dropout = 0.1, padding='same')\n",
        "    x = double_conv_module(x, 64, activation='relu', kern_reg=KR, dropout = 0.2, padding='same')\n",
        "    x = double_conv_module(x, 128, activation='relu', kern_reg=KR, dropout = 0.3, padding='same')\n",
        "    x = double_conv_module(x, 128, activation='relu', kern_reg=KR, dropout = 0.4, padding='same')\n",
        "   \n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu',kernel_regularizer=None)(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(classes)(x)\n",
        "    x = Activation(\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, x, name=\"rav_net\")\n",
        "    return model\n",
        "\n",
        "def fmp_unit(input, num_filters, dropout, padding=\"same\", frac_pool=True):\n",
        "    input = Conv2D(filters = num_filters, kernel_size = (3, 3), padding = padding, kernel_initializer='he_uniform')(input)\n",
        "    input = LeakyReLU()(input)\n",
        "    input = BatchNormalization(axis=-1)(input)\n",
        "    input = Conv2D(filters = num_filters, kernel_size = (3, 3), padding = padding, kernel_initializer='he_uniform')(input)\n",
        "    input = LeakyReLU()(input)\n",
        "    input = BatchNormalization(axis=-1)(input)\n",
        "    input = Lambda(frac_max_pool)(input) if frac_pool else input\n",
        "    input = Dropout(dropout)(input)\n",
        "\n",
        "    return input\n",
        "\n",
        "def fmp_model(width, height, depth, classes):\n",
        "    inputShape=(height, width, depth)\n",
        "    inputs = Input(shape=inputShape)\n",
        "\n",
        "    x = fmp_unit(inputs, 32, dropout = 0.3, padding='same', frac_pool=False)\n",
        "    x = fmp_unit(x, 64, dropout = 0.35, padding='same')\n",
        "    x = fmp_unit(x, 96, dropout = 0.35, padding='same')\n",
        "    x = fmp_unit(x, 128, dropout = 0.4, padding='same')\n",
        "    x = fmp_unit(x, 160, dropout = 0.45, padding='same')\n",
        "    x = fmp_unit(x, 192, dropout = 0.5, padding='same')\n",
        "\n",
        "    x = Conv2D(filters=192, kernel_size=(1, 1), padding='same', kernel_initializer='he_uniform')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(units=classes, kernel_initializer='he_uniform')(x)\n",
        "    x = Activation(\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, x, name=\"fmp_rav_net\")\n",
        "    return model\n",
        "\n",
        "def relu_bn(input):\n",
        "    relu = ReLU()(input)\n",
        "    bn = BatchNormalization()(relu)\n",
        "    return bn\n",
        "\n",
        "def residual_block(x, downsample: bool, filters: int, kernel_size: int = 3):\n",
        "    y = Conv2D(kernel_size=kernel_size, strides= (1 if not downsample else 2), filters=filters, padding=\"same\")(x)\n",
        "    y = relu_bn(y)\n",
        "    y = Conv2D(kernel_size=kernel_size, strides=1, filters=filters, padding=\"same\")(y)\n",
        "\n",
        "    x = Conv2D(kernel_size=1, strides=2, filters=filters, padding=\"same\")(x) if downsample else x\n",
        "    \n",
        "    out = Add()([x, y])\n",
        "    out = relu_bn(out)\n",
        "    return out\n",
        "\n",
        "def create_res_net(width, height, depth, classes):\n",
        "    num_filters = 64\n",
        "    inputShape=(height, width, depth)\n",
        "    inputs = Input(shape=inputShape)\n",
        "\n",
        "    t = BatchNormalization()(inputs)\n",
        "    t = Conv2D(kernel_size=3, strides=1, filters=num_filters, padding=\"same\")(t)\n",
        "    t = relu_bn(t)\n",
        "    \n",
        "    num_blocks_list = [2, 5, 5, 2]\n",
        "    for i in range(len(num_blocks_list)):\n",
        "        num_blocks = num_blocks_list[i]\n",
        "        for j in range(num_blocks):\n",
        "            t = residual_block(t, downsample=(j==0 and i!=0), filters=num_filters)\n",
        "        num_filters *= 2\n",
        "    \n",
        "    t = AveragePooling2D(4)(t)\n",
        "    x = Flatten()(t)\n",
        "    #x = Dense(512, activation='relu',kernel_regularizer=None)(x)\n",
        "    #x = BatchNormalization(axis=-1)(x)\n",
        "    #x = Dropout(0.5)(x)\n",
        "    x = Dense(classes)(x)\n",
        "    x = Activation(\"softmax\")(x)\n",
        "    \n",
        "    model = Model(inputs, x, name=\"rav_net\")\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  if DATASET == 'CIFAR10':\n",
        "    data = Data10()\n",
        "    NUM_CLASSES = 10\n",
        "  else:\n",
        "    data = Data100()\n",
        "    NUM_CLASSES = 20\n",
        "      \n",
        "  x_train, y_train = data.x_train, data.y_train\n",
        "  x_test, y_test = data.x_test, data.y_test\n",
        "  x_val, y_val = data.x_val, data.y_val\n",
        "\n",
        "  lr_scheduler = LearningRateScheduler(poly_decay)\n",
        "  variable_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor = 0.2, patience = 2)\n",
        "\n",
        "  if MODEL == \"model3x\":\n",
        "    model = rav_model(width=32, height=32, depth=3, classes=NUM_CLASSES)\n",
        "    ac='relu'\n",
        "    adm=Adam(learning_rate=0.001,decay=0, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "    opt=adm\n",
        "  \n",
        "  elif MODEL == \"fmp\":\n",
        "    model = fmp_model(width=32, height=32, depth=3, classes=NUM_CLASSES)\n",
        "    opt = RMSprop(decay=1e-6)\n",
        "  \n",
        "  elif MODEL == \"resnet\":\n",
        "    model = create_res_net(width=32, height=32, depth=3, classes=NUM_CLASSES) \n",
        "    adm=Adam(learning_rate=0.001,decay=0, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "    opt=adm\n",
        "\n",
        "  model.compile(loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'],optimizer=opt)\n",
        "  #model.compile(loss=tf.keras.losses.categorical_crossentropy, metrics=[TopKCategoricalAccuracy(k = 5)],optimizer=opt)\n",
        "  model.summary()\n",
        "  history=model.fit(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE), \n",
        "                    batch_size=BATCH_SIZE, \n",
        "                    epochs=EPOCHS, \n",
        "                    callbacks=[variable_learning_rate, lr_scheduler, stopping], \n",
        "                    validation_data=(x_val, y_val), \n",
        "                    verbose=1, \n",
        "                    steps_per_epoch = len(x_train) // BATCH_SIZE)\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  \n",
        "\n",
        "  print('Test loss:', score[0])\n",
        "  print('Test accuracy:', score[1])\n",
        "\n",
        "  # 30 epochs 82%"
      ]
    }
  ]
}